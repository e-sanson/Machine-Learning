{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Lecture5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA and Logistic Classification and Feature Development with the  MNIST image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:48.522285Z",
     "start_time": "2018-03-02T02:53:43.783853Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from E4525_ML import mnist\n",
    "from E4525_ML.multiclass_logistic import LogisticGDClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T11:06:16.497625Z",
     "start_time": "2018-02-15T11:06:16.466372Z"
    }
   },
   "source": [
    "### Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:48.537898Z",
     "start_time": "2018-03-02T02:53:48.522285Z"
    }
   },
   "outputs": [],
   "source": [
    "seed=458\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:48.561796Z",
     "start_time": "2018-03-02T02:53:48.537898Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir=r\"../../raw/mnist/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 0 </div>\n",
    "Make sure to **update** the file `mnist.py` on the `E4525_ML` directory (new version posted on Canvas).\n",
    "\n",
    "You will need the **updated** version of that file to complete the last section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.0 </div>\n",
    "Read MNIST data set and labels,  also read the MNMIST test data set and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:49.201245Z",
     "start_time": "2018-03-02T02:53:48.563202Z"
    }
   },
   "outputs": [],
   "source": [
    "X=mnist.read_images(data_dir + '/' + 'train-images-idx3-ubyte.gz')\n",
    "Y=mnist.read_labels(data_dir + '/' + 'train-labels-idx1-ubyte.gz')\n",
    "\n",
    "X_test=mnist.read_images(data_dir + '/' + 't10k-images-idx3-ubyte.gz')\n",
    "Y_test=mnist.read_labels(data_dir + '/' + 't10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:03:06.235419Z",
     "start_time": "2018-01-21T22:03:06.219795Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.2 </div>\n",
    "Use `skelearn`'s `train_test_split` function to separate the MNIST samples into  a 15% validation set and a  training sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:49.370644Z",
     "start_time": "2018-03-02T02:53:49.201245Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_validation,Y_train,Y_validation=train_test_split(X,Y,test_size=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:09:15.766264Z",
     "start_time": "2018-01-21T22:09:15.750639Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.1 </div>\n",
    "fit an LDA model on the training data set using `sklearns` `LinearDiscriminantAnalysis` classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:56.122416Z",
     "start_time": "2018-03-02T02:53:49.370644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LDA= LinearDiscriminantAnalysis()\n",
    "model_LDA.fit(X_train.reshape(X_train.shape[0],-1),Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T23:22:08.665966Z",
     "start_time": "2018-02-27T23:22:08.656899Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.2 </div>\n",
    "Compute model accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:56.250935Z",
     "start_time": "2018-03-02T02:53:56.124604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.8721176470588236\n"
     ]
    }
   ],
   "source": [
    "Y_pred=model_LDA.predict(X_train.reshape(X_train.shape[0],-1))\n",
    "print(f'The training accuracy is {np.mean(Y_pred==Y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T23:22:33.457547Z",
     "start_time": "2018-02-27T23:22:33.449526Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.3 </div>\n",
    "Compute accuracy of the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:56.284022Z",
     "start_time": "2018-03-02T02:53:56.253442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error validation set is 0.8647777777777778\n"
     ]
    }
   ],
   "source": [
    "Y_pred_1=model_LDA.predict(X_validation.reshape(X_validation.shape[0],-1))\n",
    "print(f'The error validation set is {np.mean(Y_pred_1==Y_validation)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.1 </div>\n",
    "\n",
    "Use the `LogisticGDClassifier` class from `E4525_ML.multiclass_logistic` module to fit a logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.808353Z",
     "start_time": "2018-03-02T02:53:56.286028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-11 11:46:41|\t0: TRAIN loss 2.3719,  acc 0.0700\n",
      "2019-11-11 11:46:42|\t10: TRAIN loss 0.2670,  acc 0.9200\n",
      "2019-11-11 11:46:44|\t20: TRAIN loss 0.2600,  acc 0.9240\n",
      "2019-11-11 11:46:45|\t30: TRAIN loss 0.2544,  acc 0.9260\n",
      "2019-11-11 11:46:46|\t40: TRAIN loss 0.2425,  acc 0.9240\n",
      "2019-11-11 11:46:48|\t50: TRAIN loss 0.2392,  acc 0.9350\n",
      "2019-11-11 11:46:49|\t60: TRAIN loss 0.2569,  acc 0.9240\n",
      "2019-11-11 11:46:51|\t70: TRAIN loss 0.2844,  acc 0.9220\n",
      "2019-11-11 11:46:52|\t80: TRAIN loss 0.2180,  acc 0.9440\n",
      "2019-11-11 11:46:53|\t90: TRAIN loss 0.2421,  acc 0.9460\n",
      "2019-11-11 11:46:55|\t99: TRAIN loss 0.2182,  acc 0.9360\n"
     ]
    }
   ],
   "source": [
    "model_LGDC=LogisticGDClassifier()\n",
    "model_LGDC.fit(X_train.reshape(X_train.shape[0],-1),Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.2 </div>\n",
    "Compute model accuracy in the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9344705882352942\n"
     ]
    }
   ],
   "source": [
    "Y_pred_2=model_LGDC.predict(X_train.reshape(X_train.shape[0],-1))\n",
    "print(f'The training accuracy is {np.mean(Y_pred_2==Y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T11:10:43.634394Z",
     "start_time": "2018-02-28T11:10:43.627379Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.3 </div>\n",
    "Compute model accuracy in the valuation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.941595Z",
     "start_time": "2018-03-02T02:54:18.909580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9157777777777778\n"
     ]
    }
   ],
   "source": [
    "Y_pred_3=model_LGDC.predict(X_validation.reshape(X_validation.shape[0],-1))\n",
    "print(f'The training accuracy is {np.mean(Y_pred_3==Y_validation)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering in one Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.953092Z",
     "start_time": "2018-03-02T02:54:18.945578Z"
    }
   },
   "outputs": [],
   "source": [
    "N=50\n",
    "N_val=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.968123Z",
     "start_time": "2018-03-02T02:54:18.957105Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10*(1-4*(np.abs(np.abs(x)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.988127Z",
     "start_time": "2018-03-02T02:54:18.972126Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample(N):\n",
    "    X=np.random.uniform(-2,2,N)\n",
    "    eta=f(X)\n",
    "    eta.shape\n",
    "    theta=1/(1+np.exp(-eta))\n",
    "    Y= np.random.uniform(0,1,N)>theta\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.0 </div>\n",
    "Generate \n",
    "1. a training sample of variables $X$ and $Y$ with $N$ data samples\n",
    "2. a valuation set with   $N_{val}$ samples\n",
    "3. a test set with $N_{val}$ samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.004123Z",
     "start_time": "2018-03-02T02:54:18.988127Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_1,Y_train_1=generate_sample(N)\n",
    "X_validation_1,Y_validation_1=generate_sample(N_val)\n",
    "X_test_1,Y_test_1=generate_sample(N_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T17:55:32.365942Z",
     "start_time": "2018-02-28T17:55:32.347942Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.1 </div>\n",
    "What is the proportion of positive class ($Y=1$) samples on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.032124Z",
     "start_time": "2018-03-02T02:54:19.008125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of positive class samples on the training data is 0.84.\n"
     ]
    }
   ],
   "source": [
    "print(f'The proportion of positive class samples on the training data is {list(Y_train_1).count(True)/len(Y_train_1)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T17:39:04.058942Z",
     "start_time": "2018-02-28T17:39:04.050942Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.2 </div>\n",
    "Write a function able to generate the feature matrix\n",
    "$$\n",
    "    H_{i,d}= h_d(x_i)\n",
    "$$\n",
    "for $i=1,\\dots N$ and $d=1,\\dots D$\n",
    "\n",
    "where the functions $h_d(x)$ are defined as \n",
    "$$\n",
    "    h_d(x) = x^d\n",
    "$$\n",
    "\n",
    "[HINT] be careful to include $h_D$ in the range of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.051637Z",
     "start_time": "2018-03-02T02:54:19.036124Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_features(X,D):\n",
    "    C=np.ones(D)\n",
    "    H=X[:,np.newaxis]*C[np.newaxis,:]\n",
    "    for i in range(0,D):\n",
    "        H[:,i]= H[:,i]**(i+1)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T17:42:00.116942Z",
     "start_time": "2018-02-28T17:42:00.110942Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.3 </div>\n",
    "1. Train  a logistic regression model (use sklearn `LogisticRegression` class) over the training data you already generated. \n",
    "2. Use the valuation set  to select the best value of $D$ using accuracy as selection criteria.\n",
    "3. Plot accuracy on the  training and valuation sets as a function of $D$.\n",
    "\n",
    "[HINT]\n",
    "1. You only need to consider the range $D=1,\\dots 10$.\n",
    "2. Remember to disable regularization by setting the parameter $C$ of the `LogisticRegression` class to a very large number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.300275Z",
     "start_time": "2018-03-02T02:54:19.096174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c34d75668>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Logistic_Regression=LogisticRegression(C=1e40)\n",
    "E=[]\n",
    "\n",
    "D_set=np.linspace(1,10,10,dtype=int)\n",
    "\n",
    "for D in D_set:\n",
    "    X_train_features=generate_features(X_train_1,D)\n",
    "    Logistic_Regression.fit(X_train_features,Y_train_1)\n",
    "    X_validation_features=generate_features(X_validation_1,D)\n",
    "    Y_pred=Logistic_Regression.predict(X_validation_features)\n",
    "    E.append(np.average(Y_pred==Y_validation_1))\n",
    "\n",
    "m=max(E)\n",
    "D_optimal=E.index(m)\n",
    "print(D_optimal)\n",
    "\n",
    "plt.plot(D_set,np.array(E))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.4 </div>\n",
    "Use the test set  to measure the accuracy for the optimal classifier you have found\n",
    "(do not use data from the  valuation set to train the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.312275Z",
     "start_time": "2018-03-02T02:54:19.300275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_features=generate_features(X_train_1,D_optimal)\n",
    "Logistic_Regression.fit(X_train_features,Y_train_1)\n",
    "X_test_features=generate_features(X_test_1,D_optimal)\n",
    "print(D_optimal)\n",
    "Y_pred=Logistic_Regression.predict(X_test_features)\n",
    "print(np.average(Y_pred==Y_test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for MNIST sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 5.1 </div>\n",
    "In this problem we will use `mnist.ImageFeatureModel` class to find the optimal number of orientations $\\theta$  of the oriented gradients\n",
    "features for the MNIST data set.\n",
    "\n",
    "1. use `mnist.ImageFeatureModel` to generate image oriented gradient features.\n",
    "2. use  `LogisticGDClassifier` as the base model\n",
    "3. set the block size to 4 ( this is to reduce memory use)\n",
    "4. select the best number of orientations by performing  5-Fold cross-validation on the full MNIST data set.\n",
    "5. Consider only [1,2,4,8] as possible values for the orientation\n",
    "6. Plot number of orientations vs validation accuracy\n",
    "\n",
    "[HINT] \n",
    "1. the `validation_model` function below will be useful to perform cross-validation\n",
    "2. If you run into memory trouble (your computer crashes), reduce the size of the data set.\n",
    "Make sure to  indicate this clearly on your solution.\n",
    "3. This problem is computationally expensive, make sure to allocate time to resolve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.351788Z",
     "start_time": "2018-03-02T02:54:19.312275Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_model(model,K,X,Y):\n",
    "    folder=KFold(K,shuffle=True)\n",
    "    folds=folder.split(X,Y)\n",
    "    val_error=0.0\n",
    "    fold_count=0\n",
    "    for fold in folds:\n",
    "        train_idx,val_idx=fold\n",
    "        x_train=X[train_idx]\n",
    "        y_train=Y[train_idx]\n",
    "        x_val=X[val_idx]\n",
    "        y_val=Y[val_idx]     \n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred=model.predict(x_val)\n",
    "        val_err=np.mean(y_val==y_pred)\n",
    "        val_error+=val_err\n",
    "        fold_count+=1\n",
    "        print(fold_count,val_err)\n",
    "    return val_error/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:14:27.026220Z",
     "start_time": "2018-03-02T02:54:19.356299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-11 11:47:00|\t0: TRAIN loss 2.3178,  acc 0.0970\n",
      "2019-11-11 11:47:00|\t10: TRAIN loss 0.5938,  acc 0.8220\n",
      "2019-11-11 11:47:01|\t20: TRAIN loss 0.5106,  acc 0.8310\n",
      "2019-11-11 11:47:01|\t30: TRAIN loss 0.4946,  acc 0.8420\n",
      "2019-11-11 11:47:02|\t40: TRAIN loss 0.4095,  acc 0.8710\n",
      "2019-11-11 11:47:02|\t50: TRAIN loss 0.4518,  acc 0.8630\n",
      "2019-11-11 11:47:03|\t60: TRAIN loss 0.4908,  acc 0.8500\n",
      "2019-11-11 11:47:03|\t69: TRAIN loss 0.4301,  acc 0.8570\n",
      "1 0.86275\n",
      "2019-11-11 11:47:10|\t0: TRAIN loss 2.3077,  acc 0.0920\n",
      "2019-11-11 11:47:10|\t10: TRAIN loss 0.5982,  acc 0.8240\n",
      "2019-11-11 11:47:11|\t20: TRAIN loss 0.5601,  acc 0.8350\n",
      "2019-11-11 11:47:12|\t30: TRAIN loss 0.4231,  acc 0.8740\n",
      "2019-11-11 11:47:12|\t40: TRAIN loss 0.4598,  acc 0.8540\n",
      "2019-11-11 11:47:13|\t50: TRAIN loss 0.4459,  acc 0.8670\n",
      "2019-11-11 11:47:13|\t60: TRAIN loss 0.4586,  acc 0.8600\n",
      "2019-11-11 11:47:14|\t70: TRAIN loss 0.4897,  acc 0.8390\n",
      "2019-11-11 11:47:15|\t80: TRAIN loss 0.4273,  acc 0.8660\n",
      "2019-11-11 11:47:15|\t86: TRAIN loss 0.4530,  acc 0.8710\n",
      "2 0.8600833333333333\n",
      "2019-11-11 11:47:21|\t0: TRAIN loss 2.3206,  acc 0.0940\n",
      "2019-11-11 11:47:22|\t10: TRAIN loss 0.5626,  acc 0.8420\n",
      "2019-11-11 11:47:23|\t20: TRAIN loss 0.5333,  acc 0.8260\n",
      "2019-11-11 11:47:23|\t30: TRAIN loss 0.5172,  acc 0.8420\n",
      "2019-11-11 11:47:24|\t40: TRAIN loss 0.5053,  acc 0.8460\n",
      "2019-11-11 11:47:24|\t50: TRAIN loss 0.4991,  acc 0.8540\n",
      "2019-11-11 11:47:25|\t60: TRAIN loss 0.4835,  acc 0.8490\n",
      "2019-11-11 11:47:26|\t70: TRAIN loss 0.4842,  acc 0.8550\n",
      "2019-11-11 11:47:27|\t80: TRAIN loss 0.3989,  acc 0.8830\n",
      "2019-11-11 11:47:27|\t82: TRAIN loss 0.4480,  acc 0.8780\n",
      "3 0.8623333333333333\n",
      "2019-11-11 11:47:33|\t0: TRAIN loss 2.3137,  acc 0.0820\n",
      "2019-11-11 11:47:34|\t10: TRAIN loss 0.6116,  acc 0.8280\n",
      "2019-11-11 11:47:34|\t20: TRAIN loss 0.5062,  acc 0.8500\n",
      "2019-11-11 11:47:35|\t30: TRAIN loss 0.4996,  acc 0.8590\n",
      "2019-11-11 11:47:36|\t40: TRAIN loss 0.4454,  acc 0.8600\n",
      "2019-11-11 11:47:36|\t50: TRAIN loss 0.4830,  acc 0.8500\n",
      "2019-11-11 11:47:37|\t60: TRAIN loss 0.4868,  acc 0.8440\n",
      "2019-11-11 11:47:38|\t70: TRAIN loss 0.4790,  acc 0.8590\n",
      "2019-11-11 11:47:38|\t80: TRAIN loss 0.5318,  acc 0.8420\n",
      "2019-11-11 11:47:39|\t90: TRAIN loss 0.4404,  acc 0.8690\n",
      "2019-11-11 11:47:40|\t99: TRAIN loss 0.4405,  acc 0.8590\n",
      "4 0.8599166666666667\n",
      "2019-11-11 11:47:46|\t0: TRAIN loss 2.2897,  acc 0.0980\n",
      "2019-11-11 11:47:47|\t10: TRAIN loss 0.6149,  acc 0.8200\n",
      "2019-11-11 11:47:47|\t20: TRAIN loss 0.5113,  acc 0.8460\n",
      "2019-11-11 11:47:48|\t30: TRAIN loss 0.5031,  acc 0.8400\n",
      "2019-11-11 11:47:48|\t40: TRAIN loss 0.4932,  acc 0.8570\n",
      "2019-11-11 11:47:49|\t50: TRAIN loss 0.4477,  acc 0.8560\n",
      "2019-11-11 11:47:50|\t60: TRAIN loss 0.4295,  acc 0.8710\n",
      "2019-11-11 11:47:50|\t70: TRAIN loss 0.4536,  acc 0.8660\n",
      "2019-11-11 11:47:51|\t75: TRAIN loss 0.4409,  acc 0.8660\n",
      "5 0.8595833333333334\n",
      "2019-11-11 11:47:57|\t0: TRAIN loss 2.3241,  acc 0.1580\n",
      "2019-11-11 11:47:58|\t10: TRAIN loss 0.2890,  acc 0.9220\n",
      "2019-11-11 11:47:58|\t20: TRAIN loss 0.2181,  acc 0.9390\n",
      "2019-11-11 11:47:59|\t30: TRAIN loss 0.2544,  acc 0.9320\n",
      "2019-11-11 11:48:00|\t40: TRAIN loss 0.1866,  acc 0.9440\n",
      "2019-11-11 11:48:00|\t50: TRAIN loss 0.2027,  acc 0.9360\n",
      "2019-11-11 11:48:01|\t60: TRAIN loss 0.1806,  acc 0.9470\n",
      "2019-11-11 11:48:02|\t70: TRAIN loss 0.1659,  acc 0.9540\n",
      "2019-11-11 11:48:02|\t80: TRAIN loss 0.1449,  acc 0.9590\n",
      "2019-11-11 11:48:03|\t90: TRAIN loss 0.1820,  acc 0.9510\n",
      "2019-11-11 11:48:04|\t99: TRAIN loss 0.1557,  acc 0.9590\n",
      "1 0.9466666666666667\n",
      "2019-11-11 11:48:11|\t0: TRAIN loss 2.3242,  acc 0.0680\n",
      "2019-11-11 11:48:12|\t10: TRAIN loss 0.3142,  acc 0.9150\n",
      "2019-11-11 11:48:12|\t20: TRAIN loss 0.2503,  acc 0.9410\n",
      "2019-11-11 11:48:13|\t30: TRAIN loss 0.2158,  acc 0.9400\n",
      "2019-11-11 11:48:14|\t40: TRAIN loss 0.1788,  acc 0.9570\n",
      "2019-11-11 11:48:14|\t50: TRAIN loss 0.2169,  acc 0.9420\n",
      "2019-11-11 11:48:15|\t60: TRAIN loss 0.2030,  acc 0.9450\n",
      "2019-11-11 11:48:15|\t70: TRAIN loss 0.1775,  acc 0.9520\n",
      "2019-11-11 11:48:16|\t80: TRAIN loss 0.1659,  acc 0.9480\n",
      "2019-11-11 11:48:17|\t90: TRAIN loss 0.1488,  acc 0.9660\n",
      "2019-11-11 11:48:18|\t99: TRAIN loss 0.1532,  acc 0.9550\n",
      "2 0.9500833333333333\n",
      "2019-11-11 11:48:25|\t0: TRAIN loss 2.3121,  acc 0.1330\n",
      "2019-11-11 11:48:26|\t10: TRAIN loss 0.3632,  acc 0.9020\n",
      "2019-11-11 11:48:26|\t20: TRAIN loss 0.2484,  acc 0.9320\n",
      "2019-11-11 11:48:27|\t30: TRAIN loss 0.2125,  acc 0.9470\n",
      "2019-11-11 11:48:28|\t40: TRAIN loss 0.2078,  acc 0.9390\n",
      "2019-11-11 11:48:28|\t50: TRAIN loss 0.2030,  acc 0.9360\n",
      "2019-11-11 11:48:29|\t60: TRAIN loss 0.1916,  acc 0.9470\n",
      "2019-11-11 11:48:29|\t70: TRAIN loss 0.1995,  acc 0.9390\n",
      "2019-11-11 11:48:30|\t80: TRAIN loss 0.1538,  acc 0.9480\n",
      "2019-11-11 11:48:31|\t90: TRAIN loss 0.1686,  acc 0.9480\n",
      "2019-11-11 11:48:31|\t99: TRAIN loss 0.1924,  acc 0.9410\n",
      "3 0.95\n",
      "2019-11-11 11:48:38|\t0: TRAIN loss 2.3133,  acc 0.0750\n",
      "2019-11-11 11:48:39|\t10: TRAIN loss 0.3101,  acc 0.9180\n",
      "2019-11-11 11:48:40|\t20: TRAIN loss 0.2342,  acc 0.9320\n",
      "2019-11-11 11:48:40|\t30: TRAIN loss 0.1914,  acc 0.9490\n",
      "2019-11-11 11:48:41|\t40: TRAIN loss 0.2048,  acc 0.9380\n",
      "2019-11-11 11:48:42|\t50: TRAIN loss 0.1894,  acc 0.9450\n",
      "2019-11-11 11:48:42|\t60: TRAIN loss 0.1616,  acc 0.9470\n",
      "2019-11-11 11:48:43|\t70: TRAIN loss 0.2011,  acc 0.9310\n",
      "2019-11-11 11:48:44|\t80: TRAIN loss 0.1796,  acc 0.9420\n",
      "2019-11-11 11:48:45|\t90: TRAIN loss 0.1921,  acc 0.9410\n",
      "2019-11-11 11:48:45|\t99: TRAIN loss 0.1455,  acc 0.9560\n",
      "4 0.94525\n",
      "2019-11-11 11:48:52|\t0: TRAIN loss 2.3116,  acc 0.1200\n",
      "2019-11-11 11:48:53|\t10: TRAIN loss 0.3118,  acc 0.9230\n",
      "2019-11-11 11:48:53|\t20: TRAIN loss 0.2525,  acc 0.9290\n",
      "2019-11-11 11:48:54|\t30: TRAIN loss 0.2204,  acc 0.9400\n",
      "2019-11-11 11:48:55|\t40: TRAIN loss 0.2226,  acc 0.9390\n",
      "2019-11-11 11:48:56|\t50: TRAIN loss 0.2230,  acc 0.9470\n",
      "2019-11-11 11:48:56|\t60: TRAIN loss 0.1709,  acc 0.9540\n",
      "2019-11-11 11:48:57|\t70: TRAIN loss 0.1970,  acc 0.9320\n",
      "2019-11-11 11:48:58|\t80: TRAIN loss 0.1784,  acc 0.9520\n",
      "2019-11-11 11:48:58|\t90: TRAIN loss 0.1823,  acc 0.9500\n",
      "2019-11-11 11:48:59|\t99: TRAIN loss 0.1427,  acc 0.9590\n",
      "5 0.9494166666666667\n",
      "2019-11-11 11:49:08|\t0: TRAIN loss 2.3032,  acc 0.0990\n",
      "2019-11-11 11:49:09|\t10: TRAIN loss 0.2358,  acc 0.9420\n",
      "2019-11-11 11:49:09|\t20: TRAIN loss 0.1895,  acc 0.9510\n",
      "2019-11-11 11:49:10|\t30: TRAIN loss 0.1585,  acc 0.9640\n",
      "2019-11-11 11:49:11|\t40: TRAIN loss 0.1575,  acc 0.9570\n",
      "2019-11-11 11:49:12|\t50: TRAIN loss 0.1473,  acc 0.9560\n",
      "2019-11-11 11:49:12|\t60: TRAIN loss 0.1170,  acc 0.9660\n",
      "2019-11-11 11:49:13|\t70: TRAIN loss 0.1447,  acc 0.9610\n",
      "2019-11-11 11:49:14|\t80: TRAIN loss 0.1047,  acc 0.9680\n",
      "2019-11-11 11:49:14|\t90: TRAIN loss 0.1484,  acc 0.9650\n",
      "2019-11-11 11:49:15|\t99: TRAIN loss 0.1083,  acc 0.9710\n",
      "1 0.9679166666666666\n",
      "2019-11-11 11:49:24|\t0: TRAIN loss 2.2657,  acc 0.1620\n",
      "2019-11-11 11:49:25|\t10: TRAIN loss 0.2189,  acc 0.9400\n",
      "2019-11-11 11:49:26|\t20: TRAIN loss 0.1832,  acc 0.9490\n",
      "2019-11-11 11:49:27|\t30: TRAIN loss 0.1623,  acc 0.9640\n",
      "2019-11-11 11:49:28|\t40: TRAIN loss 0.1790,  acc 0.9540\n",
      "2019-11-11 11:49:29|\t50: TRAIN loss 0.1221,  acc 0.9670\n",
      "2019-11-11 11:49:30|\t60: TRAIN loss 0.1416,  acc 0.9600\n",
      "2019-11-11 11:49:31|\t70: TRAIN loss 0.1181,  acc 0.9700\n",
      "2019-11-11 11:49:32|\t80: TRAIN loss 0.1325,  acc 0.9630\n",
      "2019-11-11 11:49:33|\t90: TRAIN loss 0.1241,  acc 0.9660\n",
      "2019-11-11 11:49:33|\t99: TRAIN loss 0.1221,  acc 0.9630\n",
      "2 0.9675833333333334\n",
      "2019-11-11 11:49:42|\t0: TRAIN loss 2.3012,  acc 0.1280\n",
      "2019-11-11 11:49:43|\t10: TRAIN loss 0.2409,  acc 0.9420\n",
      "2019-11-11 11:49:44|\t20: TRAIN loss 0.2016,  acc 0.9350\n",
      "2019-11-11 11:49:45|\t30: TRAIN loss 0.1690,  acc 0.9520\n",
      "2019-11-11 11:49:45|\t40: TRAIN loss 0.1504,  acc 0.9590\n",
      "2019-11-11 11:49:46|\t50: TRAIN loss 0.1240,  acc 0.9650\n",
      "2019-11-11 11:49:47|\t60: TRAIN loss 0.1319,  acc 0.9660\n",
      "2019-11-11 11:49:47|\t70: TRAIN loss 0.1140,  acc 0.9670\n",
      "2019-11-11 11:49:48|\t80: TRAIN loss 0.1190,  acc 0.9710\n",
      "2019-11-11 11:49:49|\t90: TRAIN loss 0.1129,  acc 0.9680\n",
      "2019-11-11 11:49:50|\t99: TRAIN loss 0.1223,  acc 0.9690\n",
      "3 0.968\n",
      "2019-11-11 11:49:59|\t0: TRAIN loss 2.3072,  acc 0.1220\n",
      "2019-11-11 11:49:59|\t10: TRAIN loss 0.2258,  acc 0.9380\n",
      "2019-11-11 11:50:00|\t20: TRAIN loss 0.1483,  acc 0.9590\n",
      "2019-11-11 11:50:01|\t30: TRAIN loss 0.1577,  acc 0.9630\n",
      "2019-11-11 11:50:02|\t40: TRAIN loss 0.1381,  acc 0.9640\n",
      "2019-11-11 11:50:02|\t50: TRAIN loss 0.1658,  acc 0.9590\n",
      "2019-11-11 11:50:03|\t60: TRAIN loss 0.1393,  acc 0.9590\n",
      "2019-11-11 11:50:04|\t70: TRAIN loss 0.1124,  acc 0.9760\n",
      "2019-11-11 11:50:05|\t80: TRAIN loss 0.1460,  acc 0.9680\n",
      "2019-11-11 11:50:06|\t90: TRAIN loss 0.0949,  acc 0.9770\n",
      "2019-11-11 11:50:07|\t99: TRAIN loss 0.1213,  acc 0.9620\n",
      "4 0.9665833333333333\n",
      "2019-11-11 11:50:17|\t0: TRAIN loss 2.3068,  acc 0.0850\n",
      "2019-11-11 11:50:18|\t10: TRAIN loss 0.2298,  acc 0.9420\n",
      "2019-11-11 11:50:18|\t20: TRAIN loss 0.1659,  acc 0.9620\n",
      "2019-11-11 11:50:19|\t30: TRAIN loss 0.1511,  acc 0.9640\n",
      "2019-11-11 11:50:20|\t40: TRAIN loss 0.1696,  acc 0.9580\n",
      "2019-11-11 11:50:20|\t50: TRAIN loss 0.1414,  acc 0.9620\n",
      "2019-11-11 11:50:21|\t60: TRAIN loss 0.1258,  acc 0.9700\n",
      "2019-11-11 11:50:22|\t70: TRAIN loss 0.1308,  acc 0.9640\n",
      "2019-11-11 11:50:23|\t80: TRAIN loss 0.1162,  acc 0.9710\n",
      "2019-11-11 11:50:23|\t90: TRAIN loss 0.1263,  acc 0.9690\n",
      "2019-11-11 11:50:24|\t99: TRAIN loss 0.1359,  acc 0.9650\n",
      "5 0.9646666666666667\n",
      "2019-11-11 11:50:36|\t0: TRAIN loss 2.3015,  acc 0.1470\n",
      "2019-11-11 11:50:37|\t10: TRAIN loss 0.1665,  acc 0.9520\n",
      "2019-11-11 11:50:38|\t20: TRAIN loss 0.1488,  acc 0.9510\n",
      "2019-11-11 11:50:39|\t30: TRAIN loss 0.1471,  acc 0.9620\n",
      "2019-11-11 11:50:40|\t40: TRAIN loss 0.1005,  acc 0.9700\n",
      "2019-11-11 11:50:41|\t50: TRAIN loss 0.0824,  acc 0.9840\n",
      "2019-11-11 11:50:42|\t60: TRAIN loss 0.0905,  acc 0.9780\n",
      "2019-11-11 11:50:43|\t70: TRAIN loss 0.0955,  acc 0.9730\n",
      "2019-11-11 11:50:44|\t80: TRAIN loss 0.1232,  acc 0.9700\n",
      "2019-11-11 11:50:45|\t90: TRAIN loss 0.0793,  acc 0.9820\n",
      "2019-11-11 11:50:47|\t99: TRAIN loss 0.1045,  acc 0.9760\n",
      "1 0.9723333333333334\n",
      "2019-11-11 11:51:00|\t0: TRAIN loss 2.3353,  acc 0.0840\n",
      "2019-11-11 11:51:01|\t10: TRAIN loss 0.1882,  acc 0.9530\n",
      "2019-11-11 11:51:02|\t20: TRAIN loss 0.1530,  acc 0.9620\n",
      "2019-11-11 11:51:03|\t30: TRAIN loss 0.1273,  acc 0.9590\n",
      "2019-11-11 11:51:04|\t40: TRAIN loss 0.1083,  acc 0.9700\n",
      "2019-11-11 11:51:05|\t50: TRAIN loss 0.1256,  acc 0.9680\n",
      "2019-11-11 11:51:06|\t60: TRAIN loss 0.1145,  acc 0.9690\n",
      "2019-11-11 11:51:07|\t70: TRAIN loss 0.0903,  acc 0.9770\n",
      "2019-11-11 11:51:08|\t80: TRAIN loss 0.1279,  acc 0.9690\n",
      "2019-11-11 11:51:09|\t90: TRAIN loss 0.0869,  acc 0.9720\n",
      "2019-11-11 11:51:10|\t99: TRAIN loss 0.0924,  acc 0.9760\n",
      "2 0.9726666666666667\n",
      "2019-11-11 11:51:25|\t0: TRAIN loss 2.2931,  acc 0.0770\n",
      "2019-11-11 11:51:26|\t10: TRAIN loss 0.1809,  acc 0.9560\n",
      "2019-11-11 11:51:27|\t20: TRAIN loss 0.1610,  acc 0.9610\n",
      "2019-11-11 11:51:29|\t30: TRAIN loss 0.0989,  acc 0.9770\n",
      "2019-11-11 11:51:30|\t40: TRAIN loss 0.1215,  acc 0.9680\n",
      "2019-11-11 11:51:31|\t50: TRAIN loss 0.1056,  acc 0.9670\n",
      "2019-11-11 11:51:33|\t60: TRAIN loss 0.1054,  acc 0.9700\n",
      "2019-11-11 11:51:34|\t70: TRAIN loss 0.1022,  acc 0.9720\n",
      "2019-11-11 11:51:35|\t80: TRAIN loss 0.0855,  acc 0.9720\n",
      "2019-11-11 11:51:36|\t90: TRAIN loss 0.1118,  acc 0.9750\n",
      "2019-11-11 11:51:37|\t99: TRAIN loss 0.1115,  acc 0.9640\n",
      "3 0.9718333333333333\n",
      "2019-11-11 11:51:51|\t0: TRAIN loss 2.3078,  acc 0.1010\n",
      "2019-11-11 11:51:53|\t10: TRAIN loss 0.1674,  acc 0.9520\n",
      "2019-11-11 11:51:54|\t20: TRAIN loss 0.1574,  acc 0.9570\n",
      "2019-11-11 11:51:55|\t30: TRAIN loss 0.1365,  acc 0.9690\n",
      "2019-11-11 11:51:56|\t40: TRAIN loss 0.1261,  acc 0.9630\n",
      "2019-11-11 11:51:57|\t50: TRAIN loss 0.1007,  acc 0.9750\n",
      "2019-11-11 11:51:58|\t60: TRAIN loss 0.0994,  acc 0.9700\n",
      "2019-11-11 11:52:00|\t70: TRAIN loss 0.1213,  acc 0.9640\n",
      "2019-11-11 11:52:01|\t80: TRAIN loss 0.1080,  acc 0.9730\n",
      "2019-11-11 11:52:02|\t90: TRAIN loss 0.0907,  acc 0.9690\n",
      "2019-11-11 11:52:03|\t99: TRAIN loss 0.0939,  acc 0.9680\n",
      "4 0.9733333333333334\n",
      "2019-11-11 11:52:17|\t0: TRAIN loss 2.3286,  acc 0.0730\n",
      "2019-11-11 11:52:18|\t10: TRAIN loss 0.1799,  acc 0.9510\n",
      "2019-11-11 11:52:19|\t20: TRAIN loss 0.1393,  acc 0.9700\n",
      "2019-11-11 11:52:20|\t30: TRAIN loss 0.1361,  acc 0.9610\n",
      "2019-11-11 11:52:21|\t40: TRAIN loss 0.1101,  acc 0.9730\n",
      "2019-11-11 11:52:22|\t50: TRAIN loss 0.1093,  acc 0.9700\n",
      "2019-11-11 11:52:23|\t60: TRAIN loss 0.1078,  acc 0.9690\n",
      "2019-11-11 11:52:24|\t70: TRAIN loss 0.0935,  acc 0.9750\n",
      "2019-11-11 11:52:25|\t80: TRAIN loss 0.1122,  acc 0.9720\n",
      "2019-11-11 11:52:26|\t90: TRAIN loss 0.0973,  acc 0.9650\n",
      "2019-11-11 11:52:28|\t99: TRAIN loss 0.1062,  acc 0.9710\n",
      "5 0.9705833333333334\n"
     ]
    }
   ],
   "source": [
    "Logistic_GDC=LogisticGDClassifier()\n",
    "base_model=Logistic_GDC\n",
    "size=3\n",
    "K=5\n",
    "d={}\n",
    "\n",
    "Orientations=[1,2,4,8]\n",
    "for orientation in Orientations:\n",
    "    model=mnist.ImageFeatureModel(Logistic_GDC,size=4,orientations=orientation)\n",
    "    a=validate_model(model,K,X,Y)\n",
    "    d[a]=orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is 0.9721499999999998 The number of orientations is 8\n"
     ]
    }
   ],
   "source": [
    "print(f'The validation accuracy is {max(d.keys())}',f'The number of orientations is {d[max(d.keys())]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 5.2 </div>\n",
    "\n",
    "Fit the model with the optimal number of orientations to the full MNIST data set and estimate its accuracy on the MNIST test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:16:52.002722Z",
     "start_time": "2018-03-02T03:14:27.259850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-11 11:52:45|\t0: TRAIN loss 2.3009,  acc 0.1030\n",
      "2019-11-11 11:52:46|\t10: TRAIN loss 0.1646,  acc 0.9520\n",
      "2019-11-11 11:52:47|\t20: TRAIN loss 0.1496,  acc 0.9590\n",
      "2019-11-11 11:52:49|\t30: TRAIN loss 0.1271,  acc 0.9730\n",
      "2019-11-11 11:52:50|\t40: TRAIN loss 0.1011,  acc 0.9760\n",
      "2019-11-11 11:52:51|\t50: TRAIN loss 0.1307,  acc 0.9600\n",
      "2019-11-11 11:52:52|\t60: TRAIN loss 0.0950,  acc 0.9710\n",
      "2019-11-11 11:52:53|\t70: TRAIN loss 0.0876,  acc 0.9820\n",
      "2019-11-11 11:52:54|\t80: TRAIN loss 0.0854,  acc 0.9760\n",
      "2019-11-11 11:52:54|\t82: TRAIN loss 0.1066,  acc 0.9710\n",
      "0.9765\n"
     ]
    }
   ],
   "source": [
    "model=mnist.ImageFeatureModel(Logistic_GDC,size=4,orientations=8)\n",
    "model.fit(X,Y)\n",
    "Y_pred=model.predict(X_test)\n",
    "print(np.average(Y_test==Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
