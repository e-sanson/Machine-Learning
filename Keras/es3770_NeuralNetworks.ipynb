{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:54.684926Z",
     "start_time": "2019-04-17T13:04:53.436825Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import E4525_ML.mnist as mnist\n",
    "from E4525_ML.notebook_utils import get_logger,LoggingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:54.688053Z",
     "start_time": "2019-04-17T13:04:54.685982Z"
    }
   },
   "outputs": [],
   "source": [
    "logger=get_logger(\"Homework7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:54.701964Z",
     "start_time": "2019-04-17T13:04:54.690528Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data_dir=\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:55.164334Z",
     "start_time": "2019-04-17T13:04:54.703650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "images_filename=raw_data_dir+\"/train-images-idx3-ubyte.gz\"\n",
    "labels_filename=raw_data_dir+\"/train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "test_images_filename=raw_data_dir+\"/t10k-images-idx3-ubyte.gz\"\n",
    "test_labels_filename=raw_data_dir+\"/t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "images=mnist.read_images(images_filename)\n",
    "labels=mnist.read_labels(labels_filename)\n",
    "\n",
    "test_images=mnist.read_images(test_images_filename)\n",
    "test_labels=mnist.read_labels(test_labels_filename)\n",
    "    \n",
    "print(images.shape,labels.shape,test_images.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:55.301894Z",
     "start_time": "2019-04-17T13:04:55.165817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28) (12000, 28, 28) (48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,Y_train,Y_val=train_test_split(images,labels,test_size=0.2,)\n",
    "\n",
    "print(X_train.shape,X_val.shape,Y_train.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:55.310718Z",
     "start_time": "2019-04-17T13:04:55.303490Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(hidden,nodes):\n",
    "    model=keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "    for i in range (0,hidden):\n",
    "        model.add(keras.layers.Dense(nodes,activation='relu'))\n",
    "    model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:55.483954Z",
     "start_time": "2019-04-17T13:04:55.311922Z"
    }
   },
   "outputs": [],
   "source": [
    "model=build_model(2,32)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:55.489957Z",
     "start_time": "2019-04-17T13:04:55.485249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_13 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# In each layer, there are respectively 25120, 1056 and 1056 trainable parameters.\n",
    "# There are 26 506 parameters in total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:04:55.496768Z",
     "start_time": "2019-04-17T13:04:55.491613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 10:23:56|\t0: TRAIN loss 0.7301,  acc 0.7454  ||  VAL loss 0.4929, acc 0.8294\n",
      "2019-12-09 10:24:02|\t10: TRAIN loss 0.3078,  acc 0.8882  ||  VAL loss 0.3705, acc 0.8693\n",
      "2019-12-09 10:24:07|\t20: TRAIN loss 0.2625,  acc 0.9038  ||  VAL loss 0.3489, acc 0.8772\n",
      "2019-12-09 10:24:13|\t30: TRAIN loss 0.2340,  acc 0.9142  ||  VAL loss 0.3499, acc 0.8788\n",
      "2019-12-09 10:24:19|\t40: TRAIN loss 0.2119,  acc 0.9219  ||  VAL loss 0.3635, acc 0.8808\n",
      "2019-12-09 10:24:25|\t50: TRAIN loss 0.1946,  acc 0.9285  ||  VAL loss 0.3804, acc 0.8759\n",
      "2019-12-09 10:24:31|\t60: TRAIN loss 0.1780,  acc 0.9346  ||  VAL loss 0.4034, acc 0.8782\n",
      "2019-12-09 10:24:37|\t70: TRAIN loss 0.1674,  acc 0.9390  ||  VAL loss 0.4605, acc 0.8717\n",
      "2019-12-09 10:24:42|\t80: TRAIN loss 0.1573,  acc 0.9419  ||  VAL loss 0.4664, acc 0.8683\n",
      "2019-12-09 10:24:48|\t90: TRAIN loss 0.1456,  acc 0.9464  ||  VAL loss 0.4918, acc 0.8689\n",
      "2019-12-09 10:24:54|\t100: TRAIN loss 0.1439,  acc 0.9470  ||  VAL loss 0.4822, acc 0.8726\n",
      "2019-12-09 10:25:00|\t110: TRAIN loss 0.1319,  acc 0.9525  ||  VAL loss 0.5455, acc 0.8715\n",
      "2019-12-09 10:25:06|\t120: TRAIN loss 0.1238,  acc 0.9555  ||  VAL loss 0.5579, acc 0.8668\n",
      "2019-12-09 10:25:11|\t130: TRAIN loss 0.1176,  acc 0.9582  ||  VAL loss 0.5809, acc 0.8659\n",
      "2019-12-09 10:25:17|\t140: TRAIN loss 0.1140,  acc 0.9595  ||  VAL loss 0.6328, acc 0.8649\n",
      "2019-12-09 10:25:22|\t149: TRAIN loss 0.1078,  acc 0.9619  ||  VAL loss 0.6564, acc 0.8695\n"
     ]
    }
   ],
   "source": [
    "result=model.fit(X_train,Y_train,batch_size=128,validation_data=(X_val,Y_val),\n",
    "                 epochs=150,verbose=0,callbacks=[LoggingCallback(10,logger)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:07:39.425733Z",
     "start_time": "2019-04-17T13:07:39.246322Z"
    }
   },
   "outputs": [],
   "source": [
    "# The accuracy of the model on the valuation set is 0.8695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T13:07:39.429949Z",
     "start_time": "2019-04-17T13:07:39.426980Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 10:25:39|\t0: TRAIN loss 0.7863,  acc 0.7405  ||  VAL loss 0.5873, acc 0.8067\n",
      "2019-12-09 10:25:43|\t10: TRAIN loss 0.4048,  acc 0.8617  ||  VAL loss 0.4365, acc 0.8487\n",
      "2019-12-09 10:25:48|\t20: TRAIN loss 0.3813,  acc 0.8679  ||  VAL loss 0.4209, acc 0.8551\n",
      "2019-12-09 10:25:52|\t30: TRAIN loss 0.3689,  acc 0.8712  ||  VAL loss 0.4226, acc 0.8543\n",
      "2019-12-09 10:25:57|\t40: TRAIN loss 0.3629,  acc 0.8725  ||  VAL loss 0.4293, acc 0.8524\n",
      "2019-12-09 10:26:01|\t50: TRAIN loss 0.3581,  acc 0.8736  ||  VAL loss 0.4273, acc 0.8518\n",
      "2019-12-09 10:26:06|\t60: TRAIN loss 0.3525,  acc 0.8758  ||  VAL loss 0.4222, acc 0.8544\n",
      "2019-12-09 10:26:10|\t70: TRAIN loss 0.3502,  acc 0.8763  ||  VAL loss 0.4264, acc 0.8547\n",
      "2019-12-09 10:26:15|\t80: TRAIN loss 0.3472,  acc 0.8766  ||  VAL loss 0.4301, acc 0.8534\n",
      "2019-12-09 10:26:19|\t90: TRAIN loss 0.3454,  acc 0.8779  ||  VAL loss 0.4304, acc 0.8537\n",
      "2019-12-09 10:26:24|\t100: TRAIN loss 0.3424,  acc 0.8794  ||  VAL loss 0.4363, acc 0.8522\n",
      "2019-12-09 10:26:28|\t110: TRAIN loss 0.3415,  acc 0.8785  ||  VAL loss 0.4393, acc 0.8504\n",
      "2019-12-09 10:26:33|\t120: TRAIN loss 0.3393,  acc 0.8798  ||  VAL loss 0.4364, acc 0.8506\n",
      "2019-12-09 10:26:37|\t130: TRAIN loss 0.3388,  acc 0.8806  ||  VAL loss 0.4499, acc 0.8506\n",
      "2019-12-09 10:26:42|\t140: TRAIN loss 0.3372,  acc 0.8812  ||  VAL loss 0.4403, acc 0.8500\n",
      "2019-12-09 10:26:46|\t149: TRAIN loss 0.3354,  acc 0.8825  ||  VAL loss 0.4507, acc 0.8486\n",
      ">>>>> 0 16 0.8485833333333334 <<<<<<<<<\n",
      "2019-12-09 10:26:47|\t0: TRAIN loss 0.7803,  acc 0.7468  ||  VAL loss 0.5771, acc 0.8132\n",
      "2019-12-09 10:26:51|\t10: TRAIN loss 0.4041,  acc 0.8609  ||  VAL loss 0.4355, acc 0.8516\n",
      "2019-12-09 10:26:56|\t20: TRAIN loss 0.3818,  acc 0.8679  ||  VAL loss 0.4247, acc 0.8537\n",
      "2019-12-09 10:27:00|\t30: TRAIN loss 0.3697,  acc 0.8711  ||  VAL loss 0.4250, acc 0.8533\n",
      "2019-12-09 10:27:05|\t40: TRAIN loss 0.3621,  acc 0.8722  ||  VAL loss 0.4257, acc 0.8526\n",
      "2019-12-09 10:27:09|\t50: TRAIN loss 0.3572,  acc 0.8740  ||  VAL loss 0.4254, acc 0.8551\n",
      "2019-12-09 10:27:14|\t60: TRAIN loss 0.3540,  acc 0.8741  ||  VAL loss 0.4283, acc 0.8543\n",
      "2019-12-09 10:27:18|\t70: TRAIN loss 0.3504,  acc 0.8761  ||  VAL loss 0.4281, acc 0.8543\n",
      "2019-12-09 10:27:23|\t80: TRAIN loss 0.3476,  acc 0.8777  ||  VAL loss 0.4285, acc 0.8559\n",
      "2019-12-09 10:27:28|\t90: TRAIN loss 0.3442,  acc 0.8785  ||  VAL loss 0.4363, acc 0.8523\n",
      "2019-12-09 10:27:32|\t100: TRAIN loss 0.3429,  acc 0.8783  ||  VAL loss 0.4314, acc 0.8523\n",
      "2019-12-09 10:27:37|\t110: TRAIN loss 0.3409,  acc 0.8789  ||  VAL loss 0.4400, acc 0.8507\n",
      "2019-12-09 10:27:42|\t120: TRAIN loss 0.3392,  acc 0.8806  ||  VAL loss 0.4476, acc 0.8509\n",
      "2019-12-09 10:27:46|\t130: TRAIN loss 0.3376,  acc 0.8809  ||  VAL loss 0.4384, acc 0.8531\n",
      "2019-12-09 10:27:50|\t140: TRAIN loss 0.3358,  acc 0.8819  ||  VAL loss 0.4471, acc 0.8511\n",
      "2019-12-09 10:27:54|\t149: TRAIN loss 0.3350,  acc 0.8815  ||  VAL loss 0.4485, acc 0.8475\n",
      ">>>>> 0 32 0.8475 <<<<<<<<<\n",
      "2019-12-09 10:27:55|\t0: TRAIN loss 0.7730,  acc 0.7469  ||  VAL loss 0.5848, acc 0.8097\n",
      "2019-12-09 10:28:00|\t10: TRAIN loss 0.4043,  acc 0.8609  ||  VAL loss 0.4322, acc 0.8511\n",
      "2019-12-09 10:28:04|\t20: TRAIN loss 0.3807,  acc 0.8690  ||  VAL loss 0.4251, acc 0.8536\n",
      "2019-12-09 10:28:09|\t30: TRAIN loss 0.3688,  acc 0.8719  ||  VAL loss 0.4260, acc 0.8543\n",
      "2019-12-09 10:28:13|\t40: TRAIN loss 0.3618,  acc 0.8725  ||  VAL loss 0.4246, acc 0.8561\n",
      "2019-12-09 10:28:18|\t50: TRAIN loss 0.3572,  acc 0.8743  ||  VAL loss 0.4210, acc 0.8552\n",
      "2019-12-09 10:28:23|\t60: TRAIN loss 0.3523,  acc 0.8768  ||  VAL loss 0.4259, acc 0.8549\n",
      "2019-12-09 10:28:27|\t70: TRAIN loss 0.3502,  acc 0.8762  ||  VAL loss 0.4234, acc 0.8550\n",
      "2019-12-09 10:28:32|\t80: TRAIN loss 0.3462,  acc 0.8776  ||  VAL loss 0.4295, acc 0.8538\n",
      "2019-12-09 10:28:36|\t90: TRAIN loss 0.3445,  acc 0.8785  ||  VAL loss 0.4290, acc 0.8533\n",
      "2019-12-09 10:28:41|\t100: TRAIN loss 0.3420,  acc 0.8794  ||  VAL loss 0.4335, acc 0.8517\n",
      "2019-12-09 10:28:45|\t110: TRAIN loss 0.3409,  acc 0.8789  ||  VAL loss 0.4344, acc 0.8512\n",
      "2019-12-09 10:28:50|\t120: TRAIN loss 0.3405,  acc 0.8793  ||  VAL loss 0.4352, acc 0.8522\n",
      "2019-12-09 10:28:54|\t130: TRAIN loss 0.3382,  acc 0.8809  ||  VAL loss 0.4395, acc 0.8504\n",
      "2019-12-09 10:28:59|\t140: TRAIN loss 0.3371,  acc 0.8799  ||  VAL loss 0.4501, acc 0.8480\n",
      "2019-12-09 10:29:03|\t149: TRAIN loss 0.3360,  acc 0.8802  ||  VAL loss 0.4463, acc 0.8485\n",
      ">>>>> 0 64 0.8485 <<<<<<<<<\n",
      "2019-12-09 10:29:04|\t0: TRAIN loss 0.7935,  acc 0.7375  ||  VAL loss 0.5861, acc 0.8075\n",
      "2019-12-09 10:29:08|\t10: TRAIN loss 0.4053,  acc 0.8610  ||  VAL loss 0.4314, acc 0.8517\n",
      "2019-12-09 10:29:13|\t20: TRAIN loss 0.3818,  acc 0.8674  ||  VAL loss 0.4227, acc 0.8523\n",
      "2019-12-09 10:29:17|\t30: TRAIN loss 0.3700,  acc 0.8707  ||  VAL loss 0.4199, acc 0.8547\n",
      "2019-12-09 10:29:22|\t40: TRAIN loss 0.3633,  acc 0.8731  ||  VAL loss 0.4206, acc 0.8560\n",
      "2019-12-09 10:29:26|\t50: TRAIN loss 0.3575,  acc 0.8750  ||  VAL loss 0.4270, acc 0.8531\n",
      "2019-12-09 10:29:31|\t60: TRAIN loss 0.3540,  acc 0.8753  ||  VAL loss 0.4352, acc 0.8505\n",
      "2019-12-09 10:29:35|\t70: TRAIN loss 0.3503,  acc 0.8763  ||  VAL loss 0.4292, acc 0.8523\n",
      "2019-12-09 10:29:40|\t80: TRAIN loss 0.3475,  acc 0.8766  ||  VAL loss 0.4350, acc 0.8539\n",
      "2019-12-09 10:29:44|\t90: TRAIN loss 0.3451,  acc 0.8797  ||  VAL loss 0.4314, acc 0.8534\n",
      "2019-12-09 10:29:49|\t100: TRAIN loss 0.3423,  acc 0.8791  ||  VAL loss 0.4408, acc 0.8482\n",
      "2019-12-09 10:29:53|\t110: TRAIN loss 0.3416,  acc 0.8798  ||  VAL loss 0.4347, acc 0.8533\n",
      "2019-12-09 10:29:57|\t120: TRAIN loss 0.3400,  acc 0.8794  ||  VAL loss 0.4380, acc 0.8520\n",
      "2019-12-09 10:30:02|\t130: TRAIN loss 0.3385,  acc 0.8798  ||  VAL loss 0.4495, acc 0.8455\n",
      "2019-12-09 10:30:06|\t140: TRAIN loss 0.3373,  acc 0.8804  ||  VAL loss 0.4417, acc 0.8500\n",
      "2019-12-09 10:30:10|\t149: TRAIN loss 0.3358,  acc 0.8807  ||  VAL loss 0.4447, acc 0.8503\n",
      ">>>>> 0 128 0.85025 <<<<<<<<<\n",
      "2019-12-09 10:30:11|\t0: TRAIN loss 0.8360,  acc 0.7134  ||  VAL loss 0.5469, acc 0.8201\n",
      "2019-12-09 10:30:16|\t10: TRAIN loss 0.3541,  acc 0.8759  ||  VAL loss 0.3970, acc 0.8615\n",
      "2019-12-09 10:30:21|\t20: TRAIN loss 0.3184,  acc 0.8867  ||  VAL loss 0.3788, acc 0.8680\n",
      "2019-12-09 10:30:26|\t30: TRAIN loss 0.3014,  acc 0.8942  ||  VAL loss 0.3745, acc 0.8699\n",
      "2019-12-09 10:30:31|\t40: TRAIN loss 0.2898,  acc 0.8964  ||  VAL loss 0.3757, acc 0.8708\n",
      "2019-12-09 10:30:36|\t50: TRAIN loss 0.2770,  acc 0.9012  ||  VAL loss 0.3929, acc 0.8675\n",
      "2019-12-09 10:30:41|\t60: TRAIN loss 0.2686,  acc 0.9046  ||  VAL loss 0.3960, acc 0.8686\n",
      "2019-12-09 10:30:46|\t70: TRAIN loss 0.2585,  acc 0.9082  ||  VAL loss 0.3932, acc 0.8689\n",
      "2019-12-09 10:30:50|\t80: TRAIN loss 0.2542,  acc 0.9104  ||  VAL loss 0.4131, acc 0.8668\n",
      "2019-12-09 10:30:55|\t90: TRAIN loss 0.2482,  acc 0.9107  ||  VAL loss 0.4112, acc 0.8673\n",
      "2019-12-09 10:31:00|\t100: TRAIN loss 0.2435,  acc 0.9132  ||  VAL loss 0.4350, acc 0.8635\n",
      "2019-12-09 10:31:04|\t110: TRAIN loss 0.2386,  acc 0.9140  ||  VAL loss 0.4276, acc 0.8635\n",
      "2019-12-09 10:31:09|\t120: TRAIN loss 0.2354,  acc 0.9152  ||  VAL loss 0.4593, acc 0.8583\n",
      "2019-12-09 10:31:14|\t130: TRAIN loss 0.2308,  acc 0.9173  ||  VAL loss 0.4482, acc 0.8620\n",
      "2019-12-09 10:31:19|\t140: TRAIN loss 0.2279,  acc 0.9187  ||  VAL loss 0.4606, acc 0.8608\n",
      "2019-12-09 10:31:23|\t149: TRAIN loss 0.2229,  acc 0.9204  ||  VAL loss 0.4680, acc 0.8597\n",
      ">>>>> 1 16 0.85975 <<<<<<<<<\n",
      "2019-12-09 10:31:24|\t0: TRAIN loss 0.6900,  acc 0.7701  ||  VAL loss 0.5006, acc 0.8288\n",
      "2019-12-09 10:31:30|\t10: TRAIN loss 0.3249,  acc 0.8859  ||  VAL loss 0.3776, acc 0.8668\n",
      "2019-12-09 10:31:35|\t20: TRAIN loss 0.2813,  acc 0.8990  ||  VAL loss 0.3605, acc 0.8741\n",
      "2019-12-09 10:31:41|\t30: TRAIN loss 0.2529,  acc 0.9084  ||  VAL loss 0.3564, acc 0.8779\n",
      "2019-12-09 10:31:46|\t40: TRAIN loss 0.2325,  acc 0.9158  ||  VAL loss 0.3680, acc 0.8767\n",
      "2019-12-09 10:31:52|\t50: TRAIN loss 0.2182,  acc 0.9209  ||  VAL loss 0.3824, acc 0.8749\n",
      "2019-12-09 10:31:58|\t60: TRAIN loss 0.2050,  acc 0.9265  ||  VAL loss 0.3922, acc 0.8758\n",
      "2019-12-09 10:32:03|\t70: TRAIN loss 0.1938,  acc 0.9297  ||  VAL loss 0.4130, acc 0.8715\n",
      "2019-12-09 10:32:09|\t80: TRAIN loss 0.1830,  acc 0.9337  ||  VAL loss 0.4113, acc 0.8751\n",
      "2019-12-09 10:32:14|\t90: TRAIN loss 0.1737,  acc 0.9369  ||  VAL loss 0.4352, acc 0.8734\n",
      "2019-12-09 10:32:20|\t100: TRAIN loss 0.1675,  acc 0.9394  ||  VAL loss 0.4520, acc 0.8708\n",
      "2019-12-09 10:32:25|\t110: TRAIN loss 0.1612,  acc 0.9416  ||  VAL loss 0.4614, acc 0.8709\n",
      "2019-12-09 10:32:31|\t120: TRAIN loss 0.1522,  acc 0.9448  ||  VAL loss 0.4855, acc 0.8709\n",
      "2019-12-09 10:32:37|\t130: TRAIN loss 0.1467,  acc 0.9487  ||  VAL loss 0.5000, acc 0.8693\n",
      "2019-12-09 10:32:42|\t140: TRAIN loss 0.1406,  acc 0.9500  ||  VAL loss 0.5203, acc 0.8687\n",
      "2019-12-09 10:32:47|\t149: TRAIN loss 0.1365,  acc 0.9520  ||  VAL loss 0.5405, acc 0.8687\n",
      ">>>>> 1 32 0.86875 <<<<<<<<<\n",
      "2019-12-09 10:32:48|\t0: TRAIN loss 0.6147,  acc 0.7934  ||  VAL loss 0.4678, acc 0.8415\n",
      "2019-12-09 10:32:56|\t10: TRAIN loss 0.2866,  acc 0.8971  ||  VAL loss 0.3356, acc 0.8798\n",
      "2019-12-09 10:33:03|\t20: TRAIN loss 0.2288,  acc 0.9166  ||  VAL loss 0.3313, acc 0.8831\n",
      "2019-12-09 10:33:10|\t30: TRAIN loss 0.1940,  acc 0.9276  ||  VAL loss 0.3497, acc 0.8802\n",
      "2019-12-09 10:33:18|\t40: TRAIN loss 0.1674,  acc 0.9378  ||  VAL loss 0.3513, acc 0.8861\n",
      "2019-12-09 10:33:25|\t50: TRAIN loss 0.1484,  acc 0.9460  ||  VAL loss 0.3820, acc 0.8833\n",
      "2019-12-09 10:33:32|\t60: TRAIN loss 0.1328,  acc 0.9509  ||  VAL loss 0.4060, acc 0.8863\n",
      "2019-12-09 10:33:40|\t70: TRAIN loss 0.1161,  acc 0.9571  ||  VAL loss 0.4354, acc 0.8827\n",
      "2019-12-09 10:33:47|\t80: TRAIN loss 0.1032,  acc 0.9630  ||  VAL loss 0.4567, acc 0.8804\n",
      "2019-12-09 10:33:54|\t90: TRAIN loss 0.0925,  acc 0.9674  ||  VAL loss 0.5150, acc 0.8763\n",
      "2019-12-09 10:34:02|\t100: TRAIN loss 0.0809,  acc 0.9714  ||  VAL loss 0.5250, acc 0.8779\n",
      "2019-12-09 10:34:09|\t110: TRAIN loss 0.0737,  acc 0.9742  ||  VAL loss 0.5736, acc 0.8741\n",
      "2019-12-09 10:34:17|\t120: TRAIN loss 0.0669,  acc 0.9759  ||  VAL loss 0.5967, acc 0.8767\n",
      "2019-12-09 10:34:24|\t130: TRAIN loss 0.0620,  acc 0.9787  ||  VAL loss 0.6176, acc 0.8772\n",
      "2019-12-09 10:34:31|\t140: TRAIN loss 0.0596,  acc 0.9792  ||  VAL loss 0.6796, acc 0.8733\n",
      "2019-12-09 10:34:37|\t149: TRAIN loss 0.0523,  acc 0.9822  ||  VAL loss 0.6928, acc 0.8745\n",
      ">>>>> 1 64 0.8745 <<<<<<<<<\n",
      "2019-12-09 10:34:39|\t0: TRAIN loss 0.5777,  acc 0.8019  ||  VAL loss 0.5051, acc 0.8243\n",
      "2019-12-09 10:34:48|\t10: TRAIN loss 0.2630,  acc 0.9043  ||  VAL loss 0.3432, acc 0.8769\n",
      "2019-12-09 10:34:57|\t20: TRAIN loss 0.2037,  acc 0.9254  ||  VAL loss 0.3523, acc 0.8802\n",
      "2019-12-09 10:35:06|\t30: TRAIN loss 0.1640,  acc 0.9407  ||  VAL loss 0.3500, acc 0.8858\n",
      "2019-12-09 10:35:15|\t40: TRAIN loss 0.1307,  acc 0.9531  ||  VAL loss 0.3812, acc 0.8887\n",
      "2019-12-09 10:35:24|\t50: TRAIN loss 0.1076,  acc 0.9605  ||  VAL loss 0.4121, acc 0.8878\n",
      "2019-12-09 10:35:33|\t60: TRAIN loss 0.0894,  acc 0.9682  ||  VAL loss 0.4498, acc 0.8874\n",
      "2019-12-09 10:35:42|\t70: TRAIN loss 0.0715,  acc 0.9749  ||  VAL loss 0.4919, acc 0.8878\n",
      "2019-12-09 10:35:51|\t80: TRAIN loss 0.0618,  acc 0.9782  ||  VAL loss 0.5292, acc 0.8852\n",
      "2019-12-09 10:36:00|\t90: TRAIN loss 0.0547,  acc 0.9800  ||  VAL loss 0.5872, acc 0.8842\n",
      "2019-12-09 10:36:09|\t100: TRAIN loss 0.0486,  acc 0.9825  ||  VAL loss 0.6283, acc 0.8836\n",
      "2019-12-09 10:36:17|\t110: TRAIN loss 0.0395,  acc 0.9858  ||  VAL loss 0.6800, acc 0.8821\n",
      "2019-12-09 10:36:26|\t120: TRAIN loss 0.0361,  acc 0.9874  ||  VAL loss 0.7048, acc 0.8802\n",
      "2019-12-09 10:36:35|\t130: TRAIN loss 0.0327,  acc 0.9886  ||  VAL loss 0.7456, acc 0.8849\n",
      "2019-12-09 10:36:44|\t140: TRAIN loss 0.0241,  acc 0.9919  ||  VAL loss 0.7865, acc 0.8847\n",
      "2019-12-09 10:36:52|\t149: TRAIN loss 0.0168,  acc 0.9947  ||  VAL loss 0.8387, acc 0.8802\n",
      ">>>>> 1 128 0.88025 <<<<<<<<<\n",
      "2019-12-09 10:36:54|\t0: TRAIN loss 0.8914,  acc 0.7063  ||  VAL loss 0.5576, acc 0.8073\n",
      "2019-12-09 10:36:59|\t10: TRAIN loss 0.3608,  acc 0.8729  ||  VAL loss 0.3947, acc 0.8611\n",
      "2019-12-09 10:37:04|\t20: TRAIN loss 0.3237,  acc 0.8857  ||  VAL loss 0.3835, acc 0.8686\n",
      "2019-12-09 10:37:08|\t30: TRAIN loss 0.2983,  acc 0.8930  ||  VAL loss 0.3809, acc 0.8672\n",
      "2019-12-09 10:37:13|\t40: TRAIN loss 0.2831,  acc 0.8985  ||  VAL loss 0.3765, acc 0.8699\n",
      "2019-12-09 10:37:18|\t50: TRAIN loss 0.2707,  acc 0.9018  ||  VAL loss 0.3889, acc 0.8697\n",
      "2019-12-09 10:37:23|\t60: TRAIN loss 0.2596,  acc 0.9066  ||  VAL loss 0.3942, acc 0.8669\n",
      "2019-12-09 10:37:28|\t70: TRAIN loss 0.2494,  acc 0.9105  ||  VAL loss 0.4032, acc 0.8677\n",
      "2019-12-09 10:37:33|\t80: TRAIN loss 0.2437,  acc 0.9118  ||  VAL loss 0.4078, acc 0.8701\n",
      "2019-12-09 10:37:38|\t90: TRAIN loss 0.2356,  acc 0.9144  ||  VAL loss 0.4433, acc 0.8630\n",
      "2019-12-09 10:37:43|\t100: TRAIN loss 0.2307,  acc 0.9153  ||  VAL loss 0.4217, acc 0.8688\n",
      "2019-12-09 10:37:48|\t110: TRAIN loss 0.2243,  acc 0.9187  ||  VAL loss 0.4395, acc 0.8677\n",
      "2019-12-09 10:37:53|\t120: TRAIN loss 0.2219,  acc 0.9185  ||  VAL loss 0.4443, acc 0.8674\n",
      "2019-12-09 10:37:57|\t130: TRAIN loss 0.2170,  acc 0.9206  ||  VAL loss 0.4511, acc 0.8672\n",
      "2019-12-09 10:38:02|\t140: TRAIN loss 0.2119,  acc 0.9221  ||  VAL loss 0.4754, acc 0.8633\n",
      "2019-12-09 10:38:07|\t149: TRAIN loss 0.2106,  acc 0.9222  ||  VAL loss 0.4842, acc 0.8653\n",
      ">>>>> 2 16 0.8653333333333333 <<<<<<<<<\n",
      "2019-12-09 10:38:08|\t0: TRAIN loss 0.7187,  acc 0.7526  ||  VAL loss 0.5114, acc 0.8248\n",
      "2019-12-09 10:38:14|\t10: TRAIN loss 0.3052,  acc 0.8882  ||  VAL loss 0.3622, acc 0.8709\n",
      "2019-12-09 10:38:20|\t20: TRAIN loss 0.2569,  acc 0.9066  ||  VAL loss 0.3466, acc 0.8763\n",
      "2019-12-09 10:38:26|\t30: TRAIN loss 0.2265,  acc 0.9172  ||  VAL loss 0.3546, acc 0.8823\n",
      "2019-12-09 10:38:31|\t40: TRAIN loss 0.2070,  acc 0.9236  ||  VAL loss 0.3723, acc 0.8792\n",
      "2019-12-09 10:38:37|\t50: TRAIN loss 0.1878,  acc 0.9306  ||  VAL loss 0.4281, acc 0.8742\n",
      "2019-12-09 10:38:43|\t60: TRAIN loss 0.1760,  acc 0.9352  ||  VAL loss 0.4246, acc 0.8807\n",
      "2019-12-09 10:38:49|\t70: TRAIN loss 0.1607,  acc 0.9410  ||  VAL loss 0.4518, acc 0.8764\n",
      "2019-12-09 10:38:55|\t80: TRAIN loss 0.1468,  acc 0.9450  ||  VAL loss 0.4887, acc 0.8786\n",
      "2019-12-09 10:39:00|\t90: TRAIN loss 0.1403,  acc 0.9482  ||  VAL loss 0.5191, acc 0.8736\n",
      "2019-12-09 10:39:06|\t100: TRAIN loss 0.1270,  acc 0.9532  ||  VAL loss 0.5531, acc 0.8747\n",
      "2019-12-09 10:39:12|\t110: TRAIN loss 0.1284,  acc 0.9530  ||  VAL loss 0.5789, acc 0.8742\n",
      "2019-12-09 10:39:18|\t120: TRAIN loss 0.1168,  acc 0.9571  ||  VAL loss 0.6103, acc 0.8714\n",
      "2019-12-09 10:39:24|\t130: TRAIN loss 0.1085,  acc 0.9609  ||  VAL loss 0.6320, acc 0.8752\n",
      "2019-12-09 10:39:29|\t140: TRAIN loss 0.1022,  acc 0.9627  ||  VAL loss 0.6809, acc 0.8702\n",
      "2019-12-09 10:39:35|\t149: TRAIN loss 0.1073,  acc 0.9606  ||  VAL loss 0.6803, acc 0.8672\n",
      ">>>>> 2 32 0.8671666666666666 <<<<<<<<<\n",
      "2019-12-09 10:39:36|\t0: TRAIN loss 0.6206,  acc 0.7880  ||  VAL loss 0.4522, acc 0.8421\n",
      "2019-12-09 10:39:44|\t10: TRAIN loss 0.2663,  acc 0.9019  ||  VAL loss 0.3346, acc 0.8793\n",
      "2019-12-09 10:39:52|\t20: TRAIN loss 0.2105,  acc 0.9216  ||  VAL loss 0.3512, acc 0.8824\n",
      "2019-12-09 10:40:00|\t30: TRAIN loss 0.1715,  acc 0.9371  ||  VAL loss 0.3576, acc 0.8871\n",
      "2019-12-09 10:40:08|\t40: TRAIN loss 0.1441,  acc 0.9461  ||  VAL loss 0.4103, acc 0.8825\n",
      "2019-12-09 10:40:16|\t50: TRAIN loss 0.1185,  acc 0.9569  ||  VAL loss 0.4486, acc 0.8818\n",
      "2019-12-09 10:40:24|\t60: TRAIN loss 0.1017,  acc 0.9622  ||  VAL loss 0.4955, acc 0.8815\n",
      "2019-12-09 10:40:32|\t70: TRAIN loss 0.0837,  acc 0.9697  ||  VAL loss 0.5403, acc 0.8866\n",
      "2019-12-09 10:40:40|\t80: TRAIN loss 0.0745,  acc 0.9731  ||  VAL loss 0.5869, acc 0.8867\n",
      "2019-12-09 10:40:50|\t90: TRAIN loss 0.0613,  acc 0.9772  ||  VAL loss 0.6686, acc 0.8774\n",
      "2019-12-09 10:40:58|\t100: TRAIN loss 0.0566,  acc 0.9791  ||  VAL loss 0.6942, acc 0.8802\n",
      "2019-12-09 10:41:06|\t110: TRAIN loss 0.0496,  acc 0.9817  ||  VAL loss 0.7404, acc 0.8825\n",
      "2019-12-09 10:41:14|\t120: TRAIN loss 0.0470,  acc 0.9830  ||  VAL loss 0.7812, acc 0.8798\n",
      "2019-12-09 10:41:22|\t130: TRAIN loss 0.0393,  acc 0.9859  ||  VAL loss 0.8444, acc 0.8786\n",
      "2019-12-09 10:41:30|\t140: TRAIN loss 0.0348,  acc 0.9869  ||  VAL loss 0.8746, acc 0.8790\n",
      "2019-12-09 10:41:37|\t149: TRAIN loss 0.0387,  acc 0.9862  ||  VAL loss 0.9250, acc 0.8758\n",
      ">>>>> 2 64 0.87575 <<<<<<<<<\n",
      "2019-12-09 10:41:39|\t0: TRAIN loss 0.5618,  acc 0.8046  ||  VAL loss 0.4287, acc 0.8475\n",
      "2019-12-09 10:41:51|\t10: TRAIN loss 0.2366,  acc 0.9115  ||  VAL loss 0.3185, acc 0.8894\n",
      "2019-12-09 10:42:02|\t20: TRAIN loss 0.1712,  acc 0.9356  ||  VAL loss 0.3673, acc 0.8860\n",
      "2019-12-09 10:42:13|\t30: TRAIN loss 0.1241,  acc 0.9539  ||  VAL loss 0.4593, acc 0.8758\n",
      "2019-12-09 10:42:26|\t40: TRAIN loss 0.0908,  acc 0.9655  ||  VAL loss 0.4636, acc 0.8923\n",
      "2019-12-09 10:42:38|\t50: TRAIN loss 0.0697,  acc 0.9731  ||  VAL loss 0.5064, acc 0.8924\n",
      "2019-12-09 10:42:49|\t60: TRAIN loss 0.0547,  acc 0.9799  ||  VAL loss 0.6045, acc 0.8888\n",
      "2019-12-09 10:43:01|\t70: TRAIN loss 0.0388,  acc 0.9863  ||  VAL loss 0.6620, acc 0.8916\n",
      "2019-12-09 10:43:14|\t80: TRAIN loss 0.0431,  acc 0.9837  ||  VAL loss 0.7504, acc 0.8878\n",
      "2019-12-09 10:43:25|\t90: TRAIN loss 0.0355,  acc 0.9875  ||  VAL loss 0.7916, acc 0.8883\n",
      "2019-12-09 10:43:37|\t100: TRAIN loss 0.0306,  acc 0.9890  ||  VAL loss 0.8276, acc 0.8918\n",
      "2019-12-09 10:43:48|\t110: TRAIN loss 0.0221,  acc 0.9918  ||  VAL loss 0.9071, acc 0.8916\n",
      "2019-12-09 10:44:00|\t120: TRAIN loss 0.0351,  acc 0.9870  ||  VAL loss 0.9039, acc 0.8878\n",
      "2019-12-09 10:44:13|\t130: TRAIN loss 0.0307,  acc 0.9896  ||  VAL loss 0.9407, acc 0.8861\n",
      "2019-12-09 10:44:25|\t140: TRAIN loss 0.0147,  acc 0.9950  ||  VAL loss 0.9756, acc 0.8878\n",
      "2019-12-09 10:44:36|\t149: TRAIN loss 0.0116,  acc 0.9959  ||  VAL loss 1.0495, acc 0.8913\n",
      ">>>>> 2 128 0.89125 <<<<<<<<<\n",
      "2019-12-09 10:44:37|\t0: TRAIN loss 0.8477,  acc 0.7031  ||  VAL loss 0.5699, acc 0.8108\n",
      "2019-12-09 10:44:43|\t10: TRAIN loss 0.3614,  acc 0.8715  ||  VAL loss 0.4045, acc 0.8567\n",
      "2019-12-09 10:44:48|\t20: TRAIN loss 0.3275,  acc 0.8805  ||  VAL loss 0.3846, acc 0.8636\n",
      "2019-12-09 10:44:55|\t30: TRAIN loss 0.3009,  acc 0.8885  ||  VAL loss 0.3891, acc 0.8597\n",
      "2019-12-09 10:45:02|\t40: TRAIN loss 0.2812,  acc 0.8966  ||  VAL loss 0.3856, acc 0.8694\n",
      "2019-12-09 10:45:07|\t50: TRAIN loss 0.2675,  acc 0.9014  ||  VAL loss 0.3849, acc 0.8677\n",
      "2019-12-09 10:45:14|\t60: TRAIN loss 0.2540,  acc 0.9072  ||  VAL loss 0.3937, acc 0.8681\n",
      "2019-12-09 10:45:20|\t70: TRAIN loss 0.2473,  acc 0.9078  ||  VAL loss 0.4145, acc 0.8670\n",
      "2019-12-09 10:45:27|\t80: TRAIN loss 0.2383,  acc 0.9125  ||  VAL loss 0.4359, acc 0.8631\n",
      "2019-12-09 10:45:33|\t90: TRAIN loss 0.2325,  acc 0.9145  ||  VAL loss 0.4334, acc 0.8673\n",
      "2019-12-09 10:45:39|\t100: TRAIN loss 0.2240,  acc 0.9166  ||  VAL loss 0.4513, acc 0.8622\n",
      "2019-12-09 10:45:45|\t110: TRAIN loss 0.2188,  acc 0.9197  ||  VAL loss 0.4663, acc 0.8622\n",
      "2019-12-09 10:45:50|\t120: TRAIN loss 0.2140,  acc 0.9210  ||  VAL loss 0.4853, acc 0.8586\n",
      "2019-12-09 10:45:55|\t130: TRAIN loss 0.2110,  acc 0.9218  ||  VAL loss 0.5007, acc 0.8618\n",
      "2019-12-09 10:46:01|\t140: TRAIN loss 0.2128,  acc 0.9216  ||  VAL loss 0.5142, acc 0.8602\n",
      "2019-12-09 10:46:05|\t149: TRAIN loss 0.2036,  acc 0.9255  ||  VAL loss 0.5323, acc 0.8593\n",
      ">>>>> 3 16 0.8593333333333333 <<<<<<<<<\n",
      "2019-12-09 10:46:07|\t0: TRAIN loss 0.6829,  acc 0.7579  ||  VAL loss 0.5267, acc 0.8158\n",
      "2019-12-09 10:46:13|\t10: TRAIN loss 0.3006,  acc 0.8902  ||  VAL loss 0.3513, acc 0.8750\n",
      "2019-12-09 10:46:19|\t20: TRAIN loss 0.2491,  acc 0.9083  ||  VAL loss 0.3591, acc 0.8738\n",
      "2019-12-09 10:46:25|\t30: TRAIN loss 0.2202,  acc 0.9176  ||  VAL loss 0.3674, acc 0.8782\n",
      "2019-12-09 10:46:31|\t40: TRAIN loss 0.1949,  acc 0.9274  ||  VAL loss 0.4143, acc 0.8748\n",
      "2019-12-09 10:46:37|\t50: TRAIN loss 0.1817,  acc 0.9322  ||  VAL loss 0.4173, acc 0.8758\n",
      "2019-12-09 10:46:43|\t60: TRAIN loss 0.1656,  acc 0.9379  ||  VAL loss 0.4449, acc 0.8772\n",
      "2019-12-09 10:46:49|\t70: TRAIN loss 0.1537,  acc 0.9426  ||  VAL loss 0.4556, acc 0.8777\n",
      "2019-12-09 10:46:56|\t80: TRAIN loss 0.1427,  acc 0.9473  ||  VAL loss 0.4943, acc 0.8790\n",
      "2019-12-09 10:47:02|\t90: TRAIN loss 0.1318,  acc 0.9510  ||  VAL loss 0.5371, acc 0.8742\n",
      "2019-12-09 10:47:08|\t100: TRAIN loss 0.1205,  acc 0.9550  ||  VAL loss 0.5745, acc 0.8750\n",
      "2019-12-09 10:47:14|\t110: TRAIN loss 0.1194,  acc 0.9554  ||  VAL loss 0.5979, acc 0.8702\n",
      "2019-12-09 10:47:20|\t120: TRAIN loss 0.1079,  acc 0.9602  ||  VAL loss 0.6626, acc 0.8710\n",
      "2019-12-09 10:47:26|\t130: TRAIN loss 0.1028,  acc 0.9630  ||  VAL loss 0.6592, acc 0.8713\n",
      "2019-12-09 10:47:32|\t140: TRAIN loss 0.1051,  acc 0.9612  ||  VAL loss 0.7057, acc 0.8720\n",
      "2019-12-09 10:47:37|\t149: TRAIN loss 0.0997,  acc 0.9635  ||  VAL loss 0.7235, acc 0.8738\n",
      ">>>>> 3 32 0.8738333333333334 <<<<<<<<<\n",
      "2019-12-09 10:47:39|\t0: TRAIN loss 0.6130,  acc 0.7862  ||  VAL loss 0.4542, acc 0.8374\n",
      "2019-12-09 10:47:48|\t10: TRAIN loss 0.2609,  acc 0.9040  ||  VAL loss 0.3391, acc 0.8814\n",
      "2019-12-09 10:47:57|\t20: TRAIN loss 0.2050,  acc 0.9222  ||  VAL loss 0.3496, acc 0.8842\n",
      "2019-12-09 10:48:07|\t30: TRAIN loss 0.1625,  acc 0.9382  ||  VAL loss 0.3612, acc 0.8880\n",
      "2019-12-09 10:48:16|\t40: TRAIN loss 0.1320,  acc 0.9514  ||  VAL loss 0.4209, acc 0.8862\n",
      "2019-12-09 10:48:25|\t50: TRAIN loss 0.1135,  acc 0.9581  ||  VAL loss 0.4990, acc 0.8821\n",
      "2019-12-09 10:48:34|\t60: TRAIN loss 0.1030,  acc 0.9622  ||  VAL loss 0.5398, acc 0.8835\n",
      "2019-12-09 10:48:42|\t70: TRAIN loss 0.0786,  acc 0.9703  ||  VAL loss 0.5822, acc 0.8861\n",
      "2019-12-09 10:48:51|\t80: TRAIN loss 0.0742,  acc 0.9718  ||  VAL loss 0.6552, acc 0.8828\n",
      "2019-12-09 10:49:01|\t90: TRAIN loss 0.0622,  acc 0.9766  ||  VAL loss 0.6994, acc 0.8747\n",
      "2019-12-09 10:49:09|\t100: TRAIN loss 0.0601,  acc 0.9771  ||  VAL loss 0.8162, acc 0.8809\n",
      "2019-12-09 10:49:18|\t110: TRAIN loss 0.0542,  acc 0.9803  ||  VAL loss 0.8311, acc 0.8823\n",
      "2019-12-09 10:49:29|\t120: TRAIN loss 0.0437,  acc 0.9834  ||  VAL loss 0.8780, acc 0.8788\n",
      "2019-12-09 10:49:38|\t130: TRAIN loss 0.0384,  acc 0.9858  ||  VAL loss 0.9196, acc 0.8795\n",
      "2019-12-09 10:49:47|\t140: TRAIN loss 0.0420,  acc 0.9849  ||  VAL loss 1.0505, acc 0.8778\n",
      "2019-12-09 10:49:55|\t149: TRAIN loss 0.0419,  acc 0.9838  ||  VAL loss 1.0132, acc 0.8799\n",
      ">>>>> 3 64 0.8799166666666667 <<<<<<<<<\n",
      "2019-12-09 10:49:57|\t0: TRAIN loss 0.5682,  acc 0.7995  ||  VAL loss 0.4191, acc 0.8506\n",
      "2019-12-09 10:50:12|\t10: TRAIN loss 0.2293,  acc 0.9144  ||  VAL loss 0.3349, acc 0.8850\n",
      "2019-12-09 10:50:25|\t20: TRAIN loss 0.1599,  acc 0.9402  ||  VAL loss 0.3505, acc 0.8918\n",
      "2019-12-09 10:50:37|\t30: TRAIN loss 0.1217,  acc 0.9536  ||  VAL loss 0.4310, acc 0.8908\n",
      "2019-12-09 10:50:48|\t40: TRAIN loss 0.0954,  acc 0.9635  ||  VAL loss 0.5129, acc 0.8914\n",
      "2019-12-09 10:51:01|\t50: TRAIN loss 0.0752,  acc 0.9715  ||  VAL loss 0.5666, acc 0.8900\n",
      "2019-12-09 10:51:14|\t60: TRAIN loss 0.0581,  acc 0.9776  ||  VAL loss 0.6744, acc 0.8929\n",
      "2019-12-09 10:51:26|\t70: TRAIN loss 0.0500,  acc 0.9813  ||  VAL loss 0.6886, acc 0.8913\n",
      "2019-12-09 10:51:39|\t80: TRAIN loss 0.0342,  acc 0.9872  ||  VAL loss 0.8090, acc 0.8929\n",
      "2019-12-09 10:51:51|\t90: TRAIN loss 0.0498,  acc 0.9816  ||  VAL loss 0.7544, acc 0.8887\n",
      "2019-12-09 10:52:04|\t100: TRAIN loss 0.0247,  acc 0.9910  ||  VAL loss 0.8955, acc 0.8914\n",
      "2019-12-09 10:52:16|\t110: TRAIN loss 0.0220,  acc 0.9918  ||  VAL loss 0.9207, acc 0.8953\n",
      "2019-12-09 10:52:28|\t120: TRAIN loss 0.0330,  acc 0.9892  ||  VAL loss 0.9719, acc 0.8855\n",
      "2019-12-09 10:52:40|\t130: TRAIN loss 0.0201,  acc 0.9929  ||  VAL loss 0.9944, acc 0.8902\n",
      "2019-12-09 10:52:52|\t140: TRAIN loss 0.0334,  acc 0.9880  ||  VAL loss 0.9635, acc 0.8950\n",
      "2019-12-09 10:53:03|\t149: TRAIN loss 0.0242,  acc 0.9913  ||  VAL loss 1.0380, acc 0.8860\n",
      ">>>>> 3 128 0.886 <<<<<<<<<\n",
      "2019-12-09 10:53:05|\t0: TRAIN loss 0.9827,  acc 0.6428  ||  VAL loss 0.5813, acc 0.8051\n",
      "2019-12-09 10:53:10|\t10: TRAIN loss 0.3478,  acc 0.8747  ||  VAL loss 0.3863, acc 0.8644\n",
      "2019-12-09 10:53:15|\t20: TRAIN loss 0.3108,  acc 0.8873  ||  VAL loss 0.3788, acc 0.8683\n",
      "2019-12-09 10:53:20|\t30: TRAIN loss 0.2870,  acc 0.8959  ||  VAL loss 0.3776, acc 0.8697\n",
      "2019-12-09 10:53:26|\t40: TRAIN loss 0.2708,  acc 0.9015  ||  VAL loss 0.3757, acc 0.8712\n",
      "2019-12-09 10:53:31|\t50: TRAIN loss 0.2609,  acc 0.9050  ||  VAL loss 0.3935, acc 0.8674\n",
      "2019-12-09 10:53:36|\t60: TRAIN loss 0.2481,  acc 0.9075  ||  VAL loss 0.3888, acc 0.8708\n",
      "2019-12-09 10:53:41|\t70: TRAIN loss 0.2408,  acc 0.9116  ||  VAL loss 0.4020, acc 0.8725\n",
      "2019-12-09 10:53:46|\t80: TRAIN loss 0.2326,  acc 0.9140  ||  VAL loss 0.4202, acc 0.8685\n",
      "2019-12-09 10:53:51|\t90: TRAIN loss 0.2261,  acc 0.9161  ||  VAL loss 0.4467, acc 0.8655\n",
      "2019-12-09 10:53:57|\t100: TRAIN loss 0.2193,  acc 0.9194  ||  VAL loss 0.4472, acc 0.8646\n",
      "2019-12-09 10:54:02|\t110: TRAIN loss 0.2176,  acc 0.9195  ||  VAL loss 0.4626, acc 0.8684\n",
      "2019-12-09 10:54:07|\t120: TRAIN loss 0.2127,  acc 0.9215  ||  VAL loss 0.4707, acc 0.8682\n",
      "2019-12-09 10:54:12|\t130: TRAIN loss 0.2053,  acc 0.9235  ||  VAL loss 0.4687, acc 0.8687\n",
      "2019-12-09 10:54:17|\t140: TRAIN loss 0.2026,  acc 0.9235  ||  VAL loss 0.4902, acc 0.8677\n",
      "2019-12-09 10:54:22|\t149: TRAIN loss 0.2003,  acc 0.9253  ||  VAL loss 0.5061, acc 0.8686\n",
      ">>>>> 4 16 0.8685833333333334 <<<<<<<<<\n",
      "2019-12-09 10:54:24|\t0: TRAIN loss 0.7024,  acc 0.7561  ||  VAL loss 0.4839, acc 0.8309\n",
      "2019-12-09 10:54:30|\t10: TRAIN loss 0.3008,  acc 0.8884  ||  VAL loss 0.3695, acc 0.8696\n",
      "2019-12-09 10:54:37|\t20: TRAIN loss 0.2508,  acc 0.9069  ||  VAL loss 0.3681, acc 0.8739\n",
      "2019-12-09 10:54:43|\t30: TRAIN loss 0.2239,  acc 0.9155  ||  VAL loss 0.3644, acc 0.8742\n",
      "2019-12-09 10:54:49|\t40: TRAIN loss 0.1972,  acc 0.9264  ||  VAL loss 0.3981, acc 0.8783\n",
      "2019-12-09 10:54:55|\t50: TRAIN loss 0.1830,  acc 0.9312  ||  VAL loss 0.4014, acc 0.8779\n",
      "2019-12-09 10:55:01|\t60: TRAIN loss 0.1680,  acc 0.9372  ||  VAL loss 0.4447, acc 0.8763\n",
      "2019-12-09 10:55:08|\t70: TRAIN loss 0.1526,  acc 0.9427  ||  VAL loss 0.4740, acc 0.8763\n",
      "2019-12-09 10:55:14|\t80: TRAIN loss 0.1515,  acc 0.9433  ||  VAL loss 0.4775, acc 0.8768\n",
      "2019-12-09 10:55:20|\t90: TRAIN loss 0.1327,  acc 0.9507  ||  VAL loss 0.5181, acc 0.8782\n",
      "2019-12-09 10:55:27|\t100: TRAIN loss 0.1247,  acc 0.9534  ||  VAL loss 0.5661, acc 0.8742\n",
      "2019-12-09 10:55:33|\t110: TRAIN loss 0.1227,  acc 0.9544  ||  VAL loss 0.6028, acc 0.8724\n",
      "2019-12-09 10:55:40|\t120: TRAIN loss 0.1124,  acc 0.9580  ||  VAL loss 0.6171, acc 0.8709\n",
      "2019-12-09 10:55:46|\t130: TRAIN loss 0.1109,  acc 0.9589  ||  VAL loss 0.6944, acc 0.8628\n",
      "2019-12-09 10:55:52|\t140: TRAIN loss 0.1043,  acc 0.9612  ||  VAL loss 0.7384, acc 0.8663\n",
      "2019-12-09 10:55:58|\t149: TRAIN loss 0.1046,  acc 0.9614  ||  VAL loss 0.7183, acc 0.8713\n",
      ">>>>> 4 32 0.8713333333333333 <<<<<<<<<\n",
      "2019-12-09 10:55:59|\t0: TRAIN loss 0.6317,  acc 0.7808  ||  VAL loss 0.4572, acc 0.8342\n",
      "2019-12-09 10:56:08|\t10: TRAIN loss 0.2570,  acc 0.9040  ||  VAL loss 0.3453, acc 0.8773\n",
      "2019-12-09 10:56:17|\t20: TRAIN loss 0.2030,  acc 0.9229  ||  VAL loss 0.3476, acc 0.8866\n",
      "2019-12-09 10:56:25|\t30: TRAIN loss 0.1638,  acc 0.9376  ||  VAL loss 0.3864, acc 0.8853\n",
      "2019-12-09 10:56:34|\t40: TRAIN loss 0.1444,  acc 0.9448  ||  VAL loss 0.4520, acc 0.8884\n",
      "2019-12-09 10:56:43|\t50: TRAIN loss 0.1143,  acc 0.9560  ||  VAL loss 0.4857, acc 0.8869\n",
      "2019-12-09 10:56:51|\t60: TRAIN loss 0.1019,  acc 0.9614  ||  VAL loss 0.5574, acc 0.8812\n",
      "2019-12-09 10:57:00|\t70: TRAIN loss 0.0861,  acc 0.9683  ||  VAL loss 0.6053, acc 0.8825\n",
      "2019-12-09 10:57:09|\t80: TRAIN loss 0.0682,  acc 0.9737  ||  VAL loss 0.6507, acc 0.8838\n",
      "2019-12-09 10:57:17|\t90: TRAIN loss 0.0664,  acc 0.9746  ||  VAL loss 0.7586, acc 0.8840\n",
      "2019-12-09 10:57:26|\t100: TRAIN loss 0.0546,  acc 0.9795  ||  VAL loss 0.8390, acc 0.8844\n",
      "2019-12-09 10:57:34|\t110: TRAIN loss 0.0519,  acc 0.9810  ||  VAL loss 0.7667, acc 0.8814\n",
      "2019-12-09 10:57:43|\t120: TRAIN loss 0.0404,  acc 0.9845  ||  VAL loss 0.8859, acc 0.8835\n",
      "2019-12-09 10:57:52|\t130: TRAIN loss 0.0443,  acc 0.9840  ||  VAL loss 0.9414, acc 0.8809\n",
      "2019-12-09 10:58:00|\t140: TRAIN loss 0.0480,  acc 0.9818  ||  VAL loss 0.9700, acc 0.8809\n",
      "2019-12-09 10:58:08|\t149: TRAIN loss 0.0336,  acc 0.9875  ||  VAL loss 1.0105, acc 0.8802\n",
      ">>>>> 4 64 0.8801666666666667 <<<<<<<<<\n",
      "2019-12-09 10:58:10|\t0: TRAIN loss 0.5804,  acc 0.7961  ||  VAL loss 0.4314, acc 0.8447\n",
      "2019-12-09 10:58:23|\t10: TRAIN loss 0.2315,  acc 0.9120  ||  VAL loss 0.3414, acc 0.8848\n",
      "2019-12-09 10:58:36|\t20: TRAIN loss 0.1664,  acc 0.9364  ||  VAL loss 0.3471, acc 0.8917\n",
      "2019-12-09 10:58:50|\t30: TRAIN loss 0.1227,  acc 0.9521  ||  VAL loss 0.4361, acc 0.8873\n",
      "2019-12-09 10:59:03|\t40: TRAIN loss 0.0982,  acc 0.9631  ||  VAL loss 0.4751, acc 0.8834\n",
      "2019-12-09 10:59:16|\t50: TRAIN loss 0.0798,  acc 0.9698  ||  VAL loss 0.5504, acc 0.8898\n",
      "2019-12-09 10:59:29|\t60: TRAIN loss 0.0655,  acc 0.9761  ||  VAL loss 0.5804, acc 0.8891\n",
      "2019-12-09 10:59:42|\t70: TRAIN loss 0.0573,  acc 0.9783  ||  VAL loss 0.6369, acc 0.8926\n",
      "2019-12-09 10:59:56|\t80: TRAIN loss 0.0482,  acc 0.9823  ||  VAL loss 0.7042, acc 0.8909\n",
      "2019-12-09 11:00:09|\t90: TRAIN loss 0.0344,  acc 0.9874  ||  VAL loss 0.8165, acc 0.8912\n",
      "2019-12-09 11:00:22|\t100: TRAIN loss 0.0371,  acc 0.9864  ||  VAL loss 0.8332, acc 0.8920\n",
      "2019-12-09 11:00:35|\t110: TRAIN loss 0.0304,  acc 0.9892  ||  VAL loss 0.9511, acc 0.8891\n",
      "2019-12-09 11:00:49|\t120: TRAIN loss 0.0317,  acc 0.9881  ||  VAL loss 0.8351, acc 0.8882\n",
      "2019-12-09 11:01:02|\t130: TRAIN loss 0.0272,  acc 0.9904  ||  VAL loss 0.8748, acc 0.8948\n",
      "2019-12-09 11:01:15|\t140: TRAIN loss 0.0294,  acc 0.9897  ||  VAL loss 0.8252, acc 0.8848\n",
      "2019-12-09 11:01:27|\t149: TRAIN loss 0.0284,  acc 0.9903  ||  VAL loss 0.8738, acc 0.8882\n",
      ">>>>> 4 128 0.8881666666666667 <<<<<<<<<\n",
      "2019-12-09 11:01:29|\t0: TRAIN loss 0.9226,  acc 0.6779  ||  VAL loss 0.5900, acc 0.7933\n",
      "2019-12-09 11:01:35|\t10: TRAIN loss 0.3642,  acc 0.8713  ||  VAL loss 0.4125, acc 0.8527\n",
      "2019-12-09 11:01:40|\t20: TRAIN loss 0.3186,  acc 0.8848  ||  VAL loss 0.3927, acc 0.8638\n",
      "2019-12-09 11:01:45|\t30: TRAIN loss 0.2908,  acc 0.8932  ||  VAL loss 0.3817, acc 0.8659\n",
      "2019-12-09 11:01:51|\t40: TRAIN loss 0.2727,  acc 0.9008  ||  VAL loss 0.3817, acc 0.8703\n",
      "2019-12-09 11:01:56|\t50: TRAIN loss 0.2616,  acc 0.9045  ||  VAL loss 0.3885, acc 0.8698\n",
      "2019-12-09 11:02:01|\t60: TRAIN loss 0.2499,  acc 0.9086  ||  VAL loss 0.4023, acc 0.8715\n",
      "2019-12-09 11:02:07|\t70: TRAIN loss 0.2421,  acc 0.9120  ||  VAL loss 0.4190, acc 0.8661\n",
      "2019-12-09 11:02:12|\t80: TRAIN loss 0.2348,  acc 0.9146  ||  VAL loss 0.4364, acc 0.8630\n",
      "2019-12-09 11:02:18|\t90: TRAIN loss 0.2286,  acc 0.9161  ||  VAL loss 0.4494, acc 0.8611\n",
      "2019-12-09 11:02:23|\t100: TRAIN loss 0.2225,  acc 0.9187  ||  VAL loss 0.4425, acc 0.8680\n",
      "2019-12-09 11:02:29|\t110: TRAIN loss 0.2159,  acc 0.9201  ||  VAL loss 0.4597, acc 0.8668\n",
      "2019-12-09 11:02:34|\t120: TRAIN loss 0.2115,  acc 0.9226  ||  VAL loss 0.4742, acc 0.8647\n",
      "2019-12-09 11:02:40|\t130: TRAIN loss 0.2128,  acc 0.9228  ||  VAL loss 0.4907, acc 0.8651\n",
      "2019-12-09 11:02:45|\t140: TRAIN loss 0.2039,  acc 0.9264  ||  VAL loss 0.5056, acc 0.8590\n",
      "2019-12-09 11:02:50|\t149: TRAIN loss 0.1989,  acc 0.9271  ||  VAL loss 0.5116, acc 0.8633\n",
      ">>>>> 5 16 0.86325 <<<<<<<<<\n",
      "2019-12-09 11:02:52|\t0: TRAIN loss 0.7706,  acc 0.7205  ||  VAL loss 0.5113, acc 0.8182\n",
      "2019-12-09 11:02:58|\t10: TRAIN loss 0.3032,  acc 0.8889  ||  VAL loss 0.3602, acc 0.8693\n",
      "2019-12-09 11:03:04|\t20: TRAIN loss 0.2572,  acc 0.9028  ||  VAL loss 0.3491, acc 0.8794\n",
      "2019-12-09 11:03:11|\t30: TRAIN loss 0.2249,  acc 0.9150  ||  VAL loss 0.3611, acc 0.8796\n",
      "2019-12-09 11:03:17|\t40: TRAIN loss 0.2049,  acc 0.9230  ||  VAL loss 0.4071, acc 0.8732\n",
      "2019-12-09 11:03:23|\t50: TRAIN loss 0.1863,  acc 0.9303  ||  VAL loss 0.4100, acc 0.8808\n",
      "2019-12-09 11:03:30|\t60: TRAIN loss 0.1704,  acc 0.9364  ||  VAL loss 0.4268, acc 0.8798\n",
      "2019-12-09 11:03:36|\t70: TRAIN loss 0.1532,  acc 0.9419  ||  VAL loss 0.4727, acc 0.8794\n",
      "2019-12-09 11:03:43|\t80: TRAIN loss 0.1480,  acc 0.9453  ||  VAL loss 0.4917, acc 0.8750\n",
      "2019-12-09 11:03:49|\t90: TRAIN loss 0.1332,  acc 0.9502  ||  VAL loss 0.5344, acc 0.8801\n",
      "2019-12-09 11:03:55|\t100: TRAIN loss 0.1235,  acc 0.9535  ||  VAL loss 0.5509, acc 0.8742\n",
      "2019-12-09 11:04:02|\t110: TRAIN loss 0.1190,  acc 0.9553  ||  VAL loss 0.6095, acc 0.8733\n",
      "2019-12-09 11:04:09|\t120: TRAIN loss 0.1100,  acc 0.9579  ||  VAL loss 0.6696, acc 0.8763\n",
      "2019-12-09 11:04:15|\t130: TRAIN loss 0.1079,  acc 0.9597  ||  VAL loss 0.6714, acc 0.8736\n",
      "2019-12-09 11:04:21|\t140: TRAIN loss 0.0970,  acc 0.9640  ||  VAL loss 0.7198, acc 0.8711\n",
      "2019-12-09 11:04:27|\t149: TRAIN loss 0.1015,  acc 0.9617  ||  VAL loss 0.7234, acc 0.8732\n",
      ">>>>> 5 32 0.8731666666666666 <<<<<<<<<\n",
      "2019-12-09 11:04:29|\t0: TRAIN loss 0.6339,  acc 0.7765  ||  VAL loss 0.4581, acc 0.8353\n",
      "2019-12-09 11:04:38|\t10: TRAIN loss 0.2663,  acc 0.8999  ||  VAL loss 0.3488, acc 0.8758\n",
      "2019-12-09 11:04:47|\t20: TRAIN loss 0.2065,  acc 0.9222  ||  VAL loss 0.3518, acc 0.8842\n",
      "2019-12-09 11:04:56|\t30: TRAIN loss 0.1688,  acc 0.9351  ||  VAL loss 0.3798, acc 0.8846\n",
      "2019-12-09 11:05:05|\t40: TRAIN loss 0.1390,  acc 0.9475  ||  VAL loss 0.5077, acc 0.8694\n",
      "2019-12-09 11:05:15|\t50: TRAIN loss 0.1183,  acc 0.9546  ||  VAL loss 0.4654, acc 0.8860\n",
      "2019-12-09 11:05:24|\t60: TRAIN loss 0.1037,  acc 0.9612  ||  VAL loss 0.5337, acc 0.8848\n",
      "2019-12-09 11:05:33|\t70: TRAIN loss 0.0871,  acc 0.9663  ||  VAL loss 0.6191, acc 0.8819\n",
      "2019-12-09 11:05:42|\t80: TRAIN loss 0.0802,  acc 0.9694  ||  VAL loss 0.6237, acc 0.8876\n",
      "2019-12-09 11:05:51|\t90: TRAIN loss 0.0676,  acc 0.9749  ||  VAL loss 0.7110, acc 0.8832\n",
      "2019-12-09 11:06:00|\t100: TRAIN loss 0.0629,  acc 0.9760  ||  VAL loss 0.7650, acc 0.8802\n",
      "2019-12-09 11:06:09|\t110: TRAIN loss 0.0706,  acc 0.9750  ||  VAL loss 0.6989, acc 0.8817\n",
      "2019-12-09 11:06:18|\t120: TRAIN loss 0.0473,  acc 0.9827  ||  VAL loss 0.8432, acc 0.8827\n",
      "2019-12-09 11:06:27|\t130: TRAIN loss 0.0546,  acc 0.9794  ||  VAL loss 0.8506, acc 0.8838\n",
      "2019-12-09 11:06:36|\t140: TRAIN loss 0.0516,  acc 0.9815  ||  VAL loss 0.8165, acc 0.8839\n",
      "2019-12-09 11:06:44|\t149: TRAIN loss 0.0445,  acc 0.9834  ||  VAL loss 0.8480, acc 0.8827\n",
      ">>>>> 5 64 0.8826666666666667 <<<<<<<<<\n",
      "2019-12-09 11:06:46|\t0: TRAIN loss 0.5774,  acc 0.7928  ||  VAL loss 0.4368, acc 0.8432\n",
      "2019-12-09 11:07:00|\t10: TRAIN loss 0.2359,  acc 0.9112  ||  VAL loss 0.3499, acc 0.8752\n",
      "2019-12-09 11:07:14|\t20: TRAIN loss 0.1759,  acc 0.9326  ||  VAL loss 0.3460, acc 0.8922\n",
      "2019-12-09 11:07:28|\t30: TRAIN loss 0.1290,  acc 0.9508  ||  VAL loss 0.3888, acc 0.8937\n",
      "2019-12-09 11:07:42|\t40: TRAIN loss 0.1033,  acc 0.9598  ||  VAL loss 0.4934, acc 0.8912\n",
      "2019-12-09 11:07:56|\t50: TRAIN loss 0.0835,  acc 0.9672  ||  VAL loss 0.5405, acc 0.8931\n",
      "2019-12-09 11:08:10|\t60: TRAIN loss 0.0723,  acc 0.9720  ||  VAL loss 0.6014, acc 0.8877\n",
      "2019-12-09 11:08:24|\t70: TRAIN loss 0.0540,  acc 0.9797  ||  VAL loss 0.6562, acc 0.8868\n",
      "2019-12-09 11:08:38|\t80: TRAIN loss 0.0536,  acc 0.9803  ||  VAL loss 0.6702, acc 0.8907\n",
      "2019-12-09 11:08:53|\t90: TRAIN loss 0.0438,  acc 0.9839  ||  VAL loss 0.7080, acc 0.8935\n",
      "2019-12-09 11:09:07|\t100: TRAIN loss 0.0430,  acc 0.9851  ||  VAL loss 0.6909, acc 0.8878\n",
      "2019-12-09 11:09:21|\t110: TRAIN loss 0.0362,  acc 0.9871  ||  VAL loss 0.7040, acc 0.8903\n",
      "2019-12-09 11:09:35|\t120: TRAIN loss 0.0332,  acc 0.9880  ||  VAL loss 0.7808, acc 0.8907\n",
      "2019-12-09 11:09:49|\t130: TRAIN loss 0.0325,  acc 0.9885  ||  VAL loss 0.8423, acc 0.8903\n",
      "2019-12-09 11:10:03|\t140: TRAIN loss 0.0239,  acc 0.9914  ||  VAL loss 0.8422, acc 0.8888\n",
      "2019-12-09 11:10:16|\t149: TRAIN loss 0.0261,  acc 0.9913  ||  VAL loss 0.8157, acc 0.8905\n",
      ">>>>> 5 128 0.8905 <<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "layers_array =[0,1,2,3,4,5]\n",
    "nodes_array = [16,32,64,128]\n",
    "for layers in layers_array:\n",
    "    for nodes in nodes_array:\n",
    "        model=build_model(layers,nodes)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        model.fit(X_train,Y_train,batch_size=128,validation_data=(X_val,Y_val),\n",
    "                         epochs=150,verbose=0,callbacks=[LoggingCallback(10,logger)])\n",
    "        Y_pred=np.argmax(model.predict(X_val),axis=1)\n",
    "        acc=np.mean(Y_pred==Y_val)\n",
    "        results.append((layers,nodes,acc))\n",
    "        print(\">>>>>\",layers,nodes,acc,\"<<<<<<<<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T14:01:04.855504Z",
     "start_time": "2019-04-17T13:07:39.431156Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T19:54:55.845656Z",
     "start_time": "2019-04-17T19:54:55.800254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best layers:  2\n",
      "Best nodes:  128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layers</th>\n",
       "      <th>nodes</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.848583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.848500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0.850250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.859750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.868750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.880250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.865333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.867167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.875750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>0.891250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.859333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.873833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.879917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.868583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.871333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.880167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.888167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.863250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.873167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.882667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.890500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layers  nodes  val_accuracy\n",
       "0        0     16      0.848583\n",
       "1        0     32      0.847500\n",
       "2        0     64      0.848500\n",
       "3        0    128      0.850250\n",
       "4        1     16      0.859750\n",
       "5        1     32      0.868750\n",
       "6        1     64      0.874500\n",
       "7        1    128      0.880250\n",
       "8        2     16      0.865333\n",
       "9        2     32      0.867167\n",
       "10       2     64      0.875750\n",
       "11       2    128      0.891250\n",
       "12       3     16      0.859333\n",
       "13       3     32      0.873833\n",
       "14       3     64      0.879917\n",
       "15       3    128      0.886000\n",
       "16       4     16      0.868583\n",
       "17       4     32      0.871333\n",
       "18       4     64      0.880167\n",
       "19       4    128      0.888167\n",
       "20       5     16      0.863250\n",
       "21       5     32      0.873167\n",
       "22       5     64      0.882667\n",
       "23       5    128      0.890500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1=np.array(results)\n",
    "df_results=pd.DataFrame(results, columns=['layers','nodes','val_accuracy']) \n",
    "\n",
    "\n",
    "# Optimal Network\n",
    "results[np.argmax(results_1[:,2])]\n",
    "best_layers=results[np.argmax(results_1[:,2])][0]\n",
    "best_nodes=results[np.argmax(results_1[:,2])][1]\n",
    "print('Best layers: ',best_layers)\n",
    "print('Best nodes: ',best_nodes)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The width of each layer seems to impact more the accuracy of the model. For a given width of 128, the model has nearly the same \n",
    "accuracy for 2 and 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T14:01:05.056127Z",
     "start_time": "2019-04-17T14:01:04.883389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 11:53:44|\t0: TRAIN loss 0.5437,  acc 0.8110  ||  VAL loss 0.4345, acc 0.8463\n",
      "2019-12-09 11:53:57|\t10: TRAIN loss 0.2339,  acc 0.9126  ||  VAL loss 0.3334, acc 0.8824\n",
      "2019-12-09 11:54:09|\t20: TRAIN loss 0.1691,  acc 0.9362  ||  VAL loss 0.3572, acc 0.8909\n",
      "2019-12-09 11:54:21|\t30: TRAIN loss 0.1254,  acc 0.9524  ||  VAL loss 0.3921, acc 0.8934\n",
      "2019-12-09 11:54:34|\t40: TRAIN loss 0.0948,  acc 0.9645  ||  VAL loss 0.4600, acc 0.8871\n",
      "2019-12-09 11:54:47|\t50: TRAIN loss 0.0738,  acc 0.9727  ||  VAL loss 0.5116, acc 0.8936\n",
      "2019-12-09 11:54:59|\t60: TRAIN loss 0.0675,  acc 0.9749  ||  VAL loss 0.6197, acc 0.8886\n",
      "2019-12-09 11:55:12|\t70: TRAIN loss 0.0567,  acc 0.9786  ||  VAL loss 0.6733, acc 0.8854\n",
      "2019-12-09 11:55:25|\t80: TRAIN loss 0.0435,  acc 0.9837  ||  VAL loss 0.7141, acc 0.8905\n",
      "2019-12-09 11:55:38|\t90: TRAIN loss 0.0392,  acc 0.9858  ||  VAL loss 0.8014, acc 0.8897\n",
      "2019-12-09 11:55:50|\t100: TRAIN loss 0.0278,  acc 0.9899  ||  VAL loss 0.8295, acc 0.8897\n",
      "2019-12-09 11:56:03|\t110: TRAIN loss 0.0291,  acc 0.9894  ||  VAL loss 0.9074, acc 0.8845\n",
      "2019-12-09 11:56:16|\t120: TRAIN loss 0.0295,  acc 0.9894  ||  VAL loss 0.9315, acc 0.8865\n",
      "2019-12-09 11:56:28|\t130: TRAIN loss 0.0186,  acc 0.9934  ||  VAL loss 0.9976, acc 0.8912\n",
      "2019-12-09 11:56:41|\t140: TRAIN loss 0.0235,  acc 0.9915  ||  VAL loss 1.1053, acc 0.8847\n",
      "2019-12-09 11:56:52|\t149: TRAIN loss 0.0206,  acc 0.9930  ||  VAL loss 1.0968, acc 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c4ad42be0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=build_model(best_layers,best_nodes)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(images,labels,batch_size=128,validation_data=(test_images,test_labels),\n",
    "                         epochs=150,verbose=0,callbacks=[LoggingCallback(10,logger)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the MNIST fashion test data is 0.8900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T14:10:57.992596Z",
     "start_time": "2019-04-17T14:10:57.979696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_39 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters on the optimal network is 118,282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
